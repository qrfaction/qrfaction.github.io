<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>qrfaction的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/qrfaction/qrfaction.github.io/"/>
  <updated>2018-12-04T00:44:10.623Z</updated>
  <id>https://github.com/qrfaction/qrfaction.github.io/</id>
  
  <author>
    <name>qrfaction</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2017CVPR RACNN</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/12/04/2017CVPR_RACNN/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/12/04/2017CVPR_RACNN/</id>
    <published>2018-12-04T00:32:42.000Z</published>
    <updated>2018-12-04T00:44:10.623Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章其实是篇比较简单的文章<br>但是其中有个梯度反传的细节很多博客都没纠清楚<br>文章里也没细讲</p><p>这篇文章主要就是利用弱监督定位去做细粒度分类<br>想法还是比较有意思的，上一次见到窗口定位的方式是在local attention</p><p>要弱监督定位需要把梯度传到坐标上，如果坐标进行取整则不能反传梯度<br>坐标的梯度更新是通过他设计的mask传回去的</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0001.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0002.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0003.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0004.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0005.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0006.jpg" alt="这里写图片描述"> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章其实是篇比较简单的文章&lt;br&gt;但是其中有个梯度反传的细节很多博客都没纠清楚&lt;br&gt;文章里也没细讲&lt;/p&gt;
&lt;p&gt;这篇文章主要就是利用弱监督定位去做细粒度分类&lt;br&gt;想法还是比较有意思的，上一次见到窗口定位的方式是在local attention&lt;/p&gt;
&lt;p&gt;要弱监
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="细粒度分类" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>之江视频问答1st比赛总结</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/12/03/%E4%B9%8B%E6%B1%9F%E8%A7%86%E9%A2%91%E9%97%AE%E7%AD%941st%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/12/03/之江视频问答1st比赛总结/</id>
    <published>2018-12-03T05:04:49.000Z</published>
    <updated>2018-12-03T05:11:32.083Z</updated>
    
    <content type="html"><![CDATA[<p>由于PPT上都是图，直接发打字比较好</p><p>PPT那么多页其实也就几段字，哪有那么多东西讲</p><ol><li><p>mp4关键帧I帧抽取，抽取I帧至40帧，不足则在I帧之间补足40帧</p></li><li><p>2017vqa冠军的模型 faster rcnn<br> (1) 抽取36个物体，受限于IO效率，物体和帧之间以帧为重<br> (2) attribute特征，把输出bbox的属性词作为特征，一是无io压力，二是视频物体种类变化小，可依据该事实通过频率提高该特征信噪比<br> 无效模型 densecap : image caption + action location<br> 本想用它解决动作类问题，无奈句子不方便通过频率筛选，提高特征质量，以及他本身效果不算特别好<br> 外加句子关键词太少</p></li><li><p>prior mask<br> 各类question答案词表分布很不一致，如颜色类问题，数量类问题<br> 很容易想到希望对每一类问题针对性处理，可通过用一个prior mask实现，将该类问题可能回答出的词标1，否则标0，<br> sigmoid激活后与之相乘，避免常识性错误，以及免去类间干扰(question类别)<br> 如此实现了特征共享，但每类问题针对性处理，且可自由扩展类别数量<br> question类别用最易于区分的关键字作为类，如what color ,how many 以及or类问题的or两边的词</p><p> 很容易有下面的改进方法(我没去做,感兴趣试试)</p><p> (1) 以2中抽取的视频attribute作为视频关键信息，通过这个抽取视频先验词表改进prior mask<br> (2) 减小类内干扰，如深红，浅红，细粒度分类无论在nlp还是cv都是个较难的任务,<br>&emsp;&emsp;但是不用细粒度即可答对，干脆放弃细粒度分类<br>&emsp;&emsp;这个问题可以抽象为用尽可能少的答案覆盖该类问题所有答案</p></li><li><p>问答框架<br> 多问多答，可变长输入<br> 问题不足五个重复补足</p><ol><li>免去冗余的五次io</li><li>带来上下文信息</li><li><p>劣势是偏固定的采样方式使他分数若不使用上下文信息会劣于一般训练方式</p><p>改进方法：用0补，重复会让特征信号加强，然后loss梯度权重加倍。。。<br>用0补的话是常量无信息，输出时可配合mask把梯度屏蔽掉</p></li></ol></li><li><p>针对分镜问题的数据增强方案<br> 找俩视频，各取一半帧数，时间轴拼接<br> 问题的话随便取出5个<br> attribute各取频率最高的一半<br> 一问一答，label直接取逻辑加</p></li><li><p>其他数据增强<br> (1) 训练时抽来的40帧随机取16帧 (帧数取太少的话不会有效果的)<br> (2) 测试时隔帧各采16帧，做test aug</p></li><li><p>模型<br> (1) 注意力使用MLB计算（其实啥注意力都差不多)<br> (2) 注意力集成机制：前几层question和video算出注意力权重后，各个问题生成的权重(16,1,1)取平均<br>&emsp;&emsp;与video(16,36,384)各帧相乘<br> (3) attribute 与 question直接算MLB加权平均<br> (4) encode用conv 1<em>1<br>&emsp;&emsp;用faster rcnn特征劣势是失去了空间局部依赖，又因此间接导致失去了物体级的时间局部依赖<br>&emsp;&emsp;rnn与cnn都不能用，采用self-attention+attention稀疏化解决跨时空依赖的想法<br>&emsp;&emsp;稀疏化方式 relu(w-(max(w)+min(w))/(1+λ)<br>&emsp;&emsp;经过调了半天λ，输出注意力矩阵后发现近乎为对角矩阵，退化为1</em>1conv，故直接用1*1conv<br> (6) video与question压成向量用co-attention(其实都差不多)<br> (7) 多问多答问答框架自带的上下文特征，其他question特征取平均与当前question拼接<br> (8) unit调到最小再调dropout（吐槽一下某人的0.95的dropout)<br> (9) 输入层bn + spatial dropout (bn在前，否则会有些shift)<br>&emsp;&emsp;分类层dropout，计算注意力前加spatial dropout<br> (10) focalloss</p></li><li><p>attention + loss 稀疏化(这部分我没用，分数不稳定，上限高下限低)<br> topk注意力，取权重前k名的取平均<br> loss稀疏化，loss最高的k个answer返回梯度，其他不反回<br> 词表太小，loss稀疏化没效果，语言模型如word2vec这种词表近万或过万的有较大效果<br> 可能物体仍然太少，所以不值得稀疏化</p></li></ol><p>参数也不算特别好，经验上关键的地方动了一下，其他没管</p><p>以前不混天池</p><p>未来几个月可能在这边待一会儿</p><p>希望各位大佬发车的时候带上我</p><p>如上</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于PPT上都是图，直接发打字比较好&lt;/p&gt;
&lt;p&gt;PPT那么多页其实也就几段字，哪有那么多东西讲&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;mp4关键帧I帧抽取，抽取I帧至40帧，不足则在I帧之间补足40帧&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2017vqa冠军的模型 faster 
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="比赛总结" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>2017ACL Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/20/taggingScheme/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/20/taggingScheme/</id>
    <published>2018-11-20T07:38:29.000Z</published>
    <updated>2018-11-20T07:44:45.418Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章内容本身其实蛮少的<br>但是意义还是较为重大的</p><p>整篇文章唯一的创新点大概在序列标注的标记模式<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/example.png" alt="这里写图片描述"><br>(1)-(2)-(3)<br>(1)内容使用BIES模式标记实体Begin, Inside, End,Single<br>(2)内容使用类别标记实体类别<br>(3)标记实体的在一对关系中的起始和终止位置</p><p>就近原则组合(2)相同，且(3)分别为1,2的实体</p><p>下面是模型部分<br>直接看图就好，没什么创新点<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/model.png" alt="这里写图片描述"> </p><p>这是loss<br>就是负类正类权重不一样，也没什么好讲的<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/loss.png" alt="这里写图片描述"> </p><p>这种方式效果也较为不错<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/result.png" alt="这里写图片描述"><br>但是弊端也较为明显，对关系重叠问题解决不太好<br>解决方式也很简单，多分类改成多标签分类就好了<br>还有一个貌似对句子拆分要求较高。</p><p>讲讲我做序列标注任务的几点感受<br>对padding十分敏感<br>sota说是crf+lstm<br>其实应该是双层lstm+crf<br>后面太慢了不过可以用cudnn加速版配自己写的mask zero</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章内容本身其实蛮少的&lt;br&gt;但是意义还是较为重大的&lt;/p&gt;
&lt;p&gt;整篇文章唯一的创新点大概在序列标注的标记模式&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/m
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="序列标注" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>同类对比CRF</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/13/%E5%90%8C%E7%B1%BB%E5%AF%B9%E6%AF%94CRF/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/13/同类对比CRF/</id>
    <published>2018-11-13T13:02:41.000Z</published>
    <updated>2018-11-13T13:11:47.150Z</updated>
    
    <content type="html"><![CDATA[<p>很早之前看CRF一直都是一堆概率图<br>一堆公式，看的晕乎乎，最后也不知道怎么算如何用<br>后来几个月前看到苏神的博客才醍醐灌醒<br>然后这几天也一直在研究crf的源码<br>其实这个东西哪里有那么复杂</p><p>同类对比是个很好的理解方式<br>以序列标注为例子<br>首先我们看看直接以softmax建模<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/13/同类对比CRF/softmax.png" alt="这里写图片描述"><br>对序列的每个输出都进行softmax输出概率P(Y<sub>i</sub>|X)<br>整个序列出现概率为P(Y|X) = ∏<sub>i</sub> P(Y<sub>i</sub>|X)<br>该序列的loss为<br>loss = -Σ<sub>i</sub> log(Y<sub>i</sub>|X)</p><p>而这种建模序列概率的方式没有考虑到序列标签的依赖关系<br>crf是考虑了这种依赖关系而对整个句子的概率进行建模<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/13/同类对比CRF/crf.png" alt="这里写图片描述"><br>P(Y|X) = exp(h(y<sub>1</sub>;x)+g(y<sub>1</sub>,y<sub>2</sub>;x)+…+h(y<sub>n</sub>;x))/Z(x)<br>P(Y|X) = exp(Σ<sub>i</sub> h(y<sub>i</sub>;x) + Σ<sub>i</sub>  g(y<sub>i</sub>,y<sub>i+1</sub>;x))/Z(x)<br>如此给句子打分就考虑到了句子间的近临依赖<br>loss = - log(P(Y|X))<br>其中h是之前rnn/cnn等的输出，而g是状态转移矩阵，可用自定义rnn实现</p><p>但是分子计算是完成了<br>分母的计算呢<br>这里用到dp即可计算得到<br>Z<sub>t+1</sub>(i) = ( Σ<sub>j</sub> Z<sub>t</sub>(j) <em> G(j|i) ) </em> H(i|X)<br>Z<sub>t+1</sub>(i)意思是t+1时刻以i结尾的所有序列得分和<br>G(j|i)为exp(g(y<sub>i</sub>,y<sub>i+1</sub>;x))<br>H(i,X)为exp(h(y<sub>i</sub>;x))</p><p>那么还有一个问题，如何预测？<br>普通的对句子的所有输出进行softmax，直接argmax就好了</p><p>用crf的话就涉及到最长路径<br>这个直接用vibiter算法即可获得</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;很早之前看CRF一直都是一堆概率图&lt;br&gt;一堆公式，看的晕乎乎，最后也不知道怎么算如何用&lt;br&gt;后来几个月前看到苏神的博客才醍醐灌醒&lt;br&gt;然后这几天也一直在研究crf的源码&lt;br&gt;其实这个东西哪里有那么复杂&lt;/p&gt;
&lt;p&gt;同类对比是个很好的理解方式&lt;br&gt;以序列标注为例子
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="序列标注" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>IoU-Net 阅读笔记</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/10/iounet/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/10/iounet/</id>
    <published>2018-11-10T05:14:37.000Z</published>
    <updated>2018-11-10T05:41:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>好久没碰检测了<br>前段时间因为一些事情又focus了几天<br>今天做个IOU-Net的简短笔记</p><p>简单来说这篇文章主要是针对这样一个问题<br>检测的目的很明确，为了获得高质量的bounding box<br>然而在以前的做法当中是先通过一系列bounding box然后再经过筛选获得<br>筛选是以分类置信度为第一优先级<br>但是，分类置信度和bbox proposal质量存在相关性但非同步增减<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/iouvsloc.png" alt="这里写图片描述"> </p><p>基于这些问题，这篇文章提出了以下策略</p><h3 id="IoU-predictor"><a href="#IoU-predictor" class="headerlink" title="IoU predictor"></a>IoU predictor</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/ioupredict.png" alt="这里写图片描述"><br>加入IoU预测分支，loss使用smooth L1<br>使用非RPN提供的proposals，而是ground truth加随机绕动得到Jittered RoIs</p><h3 id="IoU-guided-NMS"><a href="#IoU-guided-NMS" class="headerlink" title="IoU-guided NMS"></a>IoU-guided NMS</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/nms.png" alt="这里写图片描述"><br>首先选出IoU最大的bbox<br>获得与该bbox重叠率大于某阈值的bbox集合<br>将该bbox的分类置信度更新为该集合内分类置信度的最大值<br>滤掉该集合内的bbox</p><p>如此循环…</p><h3 id="Bounding-box-refinement-as-an-optimization-procedure"><a href="#Bounding-box-refinement-as-an-optimization-procedure" class="headerlink" title="Bounding box refinement as an optimization procedure"></a>Bounding box refinement as an optimization procedure</h3><p>这部分也就长话短说<br>就是我们现在有一个IoU预测器，以及检测获得的bbox<br>将这些bbox输入，获得其IoU预测值，再通过梯度反向微调bbox</p><h3 id="PrROI直接看公式吧"><a href="#PrROI直接看公式吧" class="headerlink" title="PrROI直接看公式吧"></a>PrROI直接看公式吧</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/prroi1.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/priou2.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/priou3.png" alt="这里写图片描述"><br>ROIPool -&gt; ROIAlign 避免量化<br>ROIAlign -&gt; PrROI 修补了ROIAlign不能由bin大小调整的缺陷</p><h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/result.png" alt="这里写图片描述"><br>高IoU阈值时有较显著的提升</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;好久没碰检测了&lt;br&gt;前段时间因为一些事情又focus了几天&lt;br&gt;今天做个IOU-Net的简短笔记&lt;/p&gt;
&lt;p&gt;简单来说这篇文章主要是针对这样一个问题&lt;br&gt;检测的目的很明确，为了获得高质量的bounding box&lt;br&gt;然而在以前的做法当中是先通过一系列boundi
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="目标检测" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>s2s learning as beam-search optimization</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/03/s2s/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/03/s2s/</id>
    <published>2018-11-03T13:00:51.000Z</published>
    <updated>2018-11-03T13:03:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>这周自己做的水报<br>16年的文章,感觉好像应用的并不多<br>然后文章里的loss缺陷也很明显..约束不足,感觉只适合微调<br>最近真的好忙啊…<br>争取下几周能搞点质量高的<br>反正先发上来</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_2.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_3.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_4.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_5.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_6.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_7.jpg" alt="这里写图片描述"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这周自己做的水报&lt;br&gt;16年的文章,感觉好像应用的并不多&lt;br&gt;然后文章里的loss缺陷也很明显..约束不足,感觉只适合微调&lt;br&gt;最近真的好忙啊…&lt;br&gt;争取下几周能搞点质量高的&lt;br&gt;反正先发上来&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="seq2seq" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>Memory network</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/21/memorynet/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/21/memorynet/</id>
    <published>2018-10-21T08:23:05.000Z</published>
    <updated>2018-10-21T08:29:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>nlp周报<br>这次给讨论班简述一下memory net的框架<br>ppt上传在这<br>主要口述,字有些少<br>时间限制,篇幅较短</p><p>下次超多版本的attention得好好讲,恩…</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_2.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_3.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_4.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_5.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_6.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_7.jpg" alt="这里写图片描述"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;nlp周报&lt;br&gt;这次给讨论班简述一下memory net的框架&lt;br&gt;ppt上传在这&lt;br&gt;主要口述,字有些少&lt;br&gt;时间限制,篇幅较短&lt;/p&gt;
&lt;p&gt;下次超多版本的attention得好好讲,恩…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https:
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="QA" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>2018ECCV DaSiamRPN阅读笔记</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/20/dasimarpn/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/20/dasimarpn/</id>
    <published>2018-10-20T06:22:39.000Z</published>
    <updated>2018-10-20T06:29:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文链接<br><a href="https://arxiv.org/pdf/1808.06048.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.06048.pdf</a><br>这个模型是2018VOT实时比赛的冠军,VOT2018长时比赛的亚军<br>DaSiamRPN在普通跟踪的Accuracy指标和长时跟踪的Recall指标中均排名第一</p><p>这个模型是基于他们今年之前的一个模型Siamese RPN改进得到的模型</p><h3 id="处理样本不均衡策略"><a href="#处理样本不均衡策略" class="headerlink" title="处理样本不均衡策略"></a>处理样本不均衡策略</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/problem.jpg" alt="这里写图片描述"><br>作者发现在跟踪过程中跟踪器对实例分类困难<br>而对前背景分类能力较强<br>而造成这个问题的原因他归因为跟踪过程中样本不均衡<br>正样本实例种类不够多模型泛化能力差</p><p>作者在训练过程中加入了如下所示的样本对进行离线训练<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/aug.jpg" alt="这里写图片描述"> </p><h3 id="增量学习方式"><a href="#增量学习方式" class="headerlink" title="增量学习方式"></a>增量学习方式</h3><p>公式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/update.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/update2.jpg" alt="这里写图片描述"><br>与模板帧的匹配分数 - 与干扰物的匹配分数作为最终分数<br>这些抗干扰物选择方式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/distractor.jpg" alt="这里写图片描述"><br>选择与模板帧相似度大于某个阈值的错误实例</p><p>再进行一般化<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/update3.jpg" alt="这里写图片描述"> </p><p>参数设置细节见论文</p><h3 id="各组件收益"><a href="#各组件收益" class="headerlink" title="各组件收益"></a>各组件收益</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/gain.jpg" alt="这里写图片描述"> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原文链接&lt;br&gt;&lt;a href=&quot;https://arxiv.org/pdf/1808.06048.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1808.06048.pdf&lt;/a&gt;&lt;br&gt;这个模型是20
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="tracking" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>sparsemax</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/18/sparsemax/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/18/sparsemax/</id>
    <published>2018-10-18T08:30:30.000Z</published>
    <updated>2018-10-18T08:56:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是稀疏性的需求而提出了一个带有稀疏特性的归一化函数sparsemax<br>常见的有 sigmoid/tanh -&gt; hard_sigmoid/hard_tanh<br>sparsemax在二维情况即为hard_sigmoid</p><p>我认为稀疏特性是attention以后十分重要的发展方向<br>目前大部分的soft attention都是基于softmax<br>这就带来了一个缺点,每个元素都会对结果产生影响<br>而hard attention又带了难以优化的问题<br>自然而然sparse attention是一个很好的发展方向<br>虽然本篇文章我应用效果并不怎样…<br>但是仍然感觉很有启发意义</p><p>本篇文章稀疏化的方式是通过一个阈值<br>再利用max函数屏蔽掉一部分权重</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/18/sparsemax/sparsemax.jpg" alt="这里写图片描述"><br>其中[x]<sub>+</sub>指代max(x,0)<br>该阈值的好处</p><ol><li>介于min(z)与max(z)之间,保证能屏蔽一部分与留下一部分</li><li>k的选择保证了主要成分的保留</li></ol></blockquote><p>此函数需自定义梯度回传公式<br>梯度回传公式如下</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/18/sparsemax/thres.jpg" alt="这里写图片描述"><br>其中S(z)为[z-r(z)]<sub>+</sub>中非0的部分<br>这个梯度公式意思很简单<br>即用到的部分回传梯度,没用到的部分梯度为0</p></blockquote><p>sparsemax应用场景</p><ol><li>attention归一化权重</li><li>多分类</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章主要是稀疏性的需求而提出了一个带有稀疏特性的归一化函数sparsemax&lt;br&gt;常见的有 sigmoid/tanh -&amp;gt; hard_sigmoid/hard_tanh&lt;br&gt;sparsemax在二维情况即为hard_sigmoid&lt;/p&gt;
&lt;p&gt;我认为稀疏特性
      
    
    </summary>
    
      <category term="ML&amp;DL" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/ML-DL/"/>
    
    
      <category term="attention" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>本年爆款... Bidirectional Encoder Representations from Transformers</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/14/bert/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/14/bert/</id>
    <published>2018-10-14T08:11:28.000Z</published>
    <updated>2018-10-14T08:19:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>这几天被BERT刷屏了<br>结果是真的好看,刷新了十一项记录,每项都有巨大改进…<br>下面分析一下这篇文章的工作</p><h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/14/bert/input.jpg" alt="这里写图片描述"><br>一个句子对,即两个句子</p><ol><li>WordPiece Embedding<br>&emsp;这个东西是用来解决oov的word的,将部分单词拆成两个字词,如fued拆成fu,ed .<br>&emsp;具体怎么拆,拆哪些,用贪心算法搜索尽可能少的token去覆盖所有单词</li><li>Segment Embedding<br>&emsp;区分是句子一还是句子二</li><li>Position Embedding<br>&emsp;融入位置信息,学习得到</li><li>cls是句子的类别信息,用于task2和其他任务(非分类任务可无视)</li><li>[SEP]是句子的分隔符<br><br></li></ol><p>下面是该论文的自监督任务的两个创新点</p><h3 id="TASK1-MLM-masked-language-model"><a href="#TASK1-MLM-masked-language-model" class="headerlink" title="TASK1 #: MLM(masked language model)"></a>TASK1 #: MLM(masked language model)</h3><p>随机屏蔽batchsize samples中一定量的单词,并去预测他 (完形填空)</p><ol><li>使用bidirectional self-attention<br>&emsp;不使用rnn可能是因为网络很深不好训练.<br>&emsp;作者认为用单项模型然后两向使用再拼接不如直接双向来的自然,能更好的捕捉上下文信息</li><li>每次屏蔽每个句子中15%的单词<br>&emsp;(1)80%的概率将单词换为[mask]标记 , my dog is hairy → my dog is [MASK]<br>&emsp;(2)10%的概率将单词换为字表中其他单词 , my dog is hairy → my dog is apple<br>&emsp;(3)10%的概率不替换 , my dog is hairy → my dog is hairy<br>具体为什么不是很清楚..   (2)可能是加噪缓解过拟合,(3)不知道了…<br><br></li></ol><h3 id="TASK2-NSP-Next-Sentence-Prediction"><a href="#TASK2-NSP-Next-Sentence-Prediction" class="headerlink" title="TASK2 #: NSP(Next Sentence Prediction)"></a>TASK2 #: NSP(Next Sentence Prediction)</h3><p>由于输入的是一个句子对,所以这个任务是去判断句子二是否可作为句子一的下一句<br>这个任务的目的应该是学习句子之间的逻辑关系</p><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p>下图是在其他任务的微调方式<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/14/bert/fune.jpg" alt="这里写图片描述"> </p><p>由于每次只屏蔽一部分<br>这比left2right收敛慢很多<br>一般来讲自监督的模型一般有比较好的泛化效果<br>然后结果也很惊人…</p><p>最后就是模型好大…用不起用不起…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这几天被BERT刷屏了&lt;br&gt;结果是真的好看,刷新了十一项记录,每项都有巨大改进…&lt;br&gt;下面分析一下这篇文章的工作&lt;/p&gt;
&lt;h3 id=&quot;Input&quot;&gt;&lt;a href=&quot;#Input&quot; class=&quot;headerlink&quot; title=&quot;Input&quot;&gt;&lt;/a&gt;Input
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="representation learning" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/representation-learning/"/>
    
  </entry>
  
  <entry>
    <title>NAACL2018 best paper ELMo</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/13/elmo/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/13/elmo/</id>
    <published>2018-10-13T07:23:00.000Z</published>
    <updated>2018-10-13T08:00:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>ELMo 是NAACL2018的best paper<br>早就想读了了,攒着一直没读…</p><p>其实nn的文章看图能识个大概了,接着再细读其中细节<br>但这篇文章没图…</p><p>下面上一张自制的</p><h3 id="ELMo结构与使用方式"><a href="#ELMo结构与使用方式" class="headerlink" title="ELMo结构与使用方式"></a>ELMo结构与使用方式</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/elmo.jpg" alt="这里写图片描述"> </p><p>如图,上面是ELMo的使用方式<br>将模型中word在LSTM中输出的中间态作为他的embedding</p><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/target.jpg" alt="这里写图片描述"><br>他自己则是一个多层双向语言自监督模型<br>正向预测和逆向预测word作为task,进行训练<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/loss.jpg" alt="这里写图片描述"> </p><p>这种设计个人认为有如下几点好处</p><ol><li>word在不同语境有不同意思,使用LSTM中间状态带入了上下文信息解决了语义歧义的问题</li><li>biLSTM相比Glove,word2vec带入了语序信息</li><li>biLSTM能捕捉一定的语法结构信息</li></ol><h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><p>冷冻biLM模型<br>将他各个中间层的信息加权平均,再和普通的词向量concat<br>公式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/w.jpg" alt="这里写图片描述"> </p><p>s<sub>task</sub>是可训练且经过softmax归一化的权重<br>s<sub>task</sub>用于在不同任务下自适应调整高维还是低维的抽象信息<br>γ是缩放因子,对模型影响较大,可训练</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ELMo 是NAACL2018的best paper&lt;br&gt;早就想读了了,攒着一直没读…&lt;/p&gt;
&lt;p&gt;其实nn的文章看图能识个大概了,接着再细读其中细节&lt;br&gt;但这篇文章没图…&lt;/p&gt;
&lt;p&gt;下面上一张自制的&lt;/p&gt;
&lt;h3 id=&quot;ELMo结构与使用方式&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="representation learning" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/representation-learning/"/>
    
  </entry>
  
  <entry>
    <title>Object Context Network for Scene Parsing</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/02/OCnet/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/02/OCnet/</id>
    <published>2018-10-02T14:34:20.000Z</published>
    <updated>2018-10-05T07:22:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇应用了attention在图像分割的文章<br>文章本身很简单,感觉工作不多<br>最近在vqa工作中对attention体会很深,也创新了不少东西<br>在结束后再写篇博文吧</p><p>回到正文</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/02/OCnet/total.jpg" alt="这里写图片描述"> </p><p>恩 … 整体框架还是无特别大的创新</p><h2 id="Object-Context"><a href="#Object-Context" class="headerlink" title="Object Context"></a>Object Context</h2><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/02/OCnet/attention.jpg" alt="这里写图片描述"> </p><p>输出的C指context<br>P指Position embedding<br>X+P将位置信息融入feature map<br>再做个很普通的attention<br>就是以像素为单位,以余弦相关性作为相似度度量.<br>获得了context他的使用方式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/02/OCnet/oc.jpg" alt="这里写图片描述"><br>配合hypercolumn或ASP</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>其实感觉Context插入的很强行<br>self-attention其实有跨大距离依赖效果才比conv强的<br>相比于self-attention<br>分割这种感受野任务,给channel加attention(不同感受野feature map拼接后权重不同)效果更佳<br>效果也只有几个千分点的提升,表示质疑<br>朋友测试下效果也不是很好…</p><p>而且他的注意力是直接用softmax的<br>对于这么多像素做softmax  有效信息被无效信息覆盖的情况一般都很严重…<br>我严重怀疑这个和average效果差不多<br>他可以试试半hard半soft的attention</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是一篇应用了attention在图像分割的文章&lt;br&gt;文章本身很简单,感觉工作不多&lt;br&gt;最近在vqa工作中对attention体会很深,也创新了不少东西&lt;br&gt;在结束后再写篇博文吧&lt;/p&gt;
&lt;p&gt;回到正文&lt;/p&gt;
&lt;h2 id=&quot;整体架构&quot;&gt;&lt;a href=&quot;#整体架构
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="图像分割" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>trick和创新点的碎碎念</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/09/09/trick%E5%92%8C%E5%88%9B%E6%96%B0%E7%82%B9%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/09/09/trick和创新点的碎碎念/</id>
    <published>2018-09-09T03:00:10.000Z</published>
    <updated>2018-09-09T03:01:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在搞video qa<br>这是一个比较新的方向,论文总共找到不到三篇,天池的视频问答比赛似乎也是全球第一届</p><p>大家都是新手,一起开荒<br>前天和鹏哥聊着聊着看他有啥想法 .<br>最后聊到trick和创新度的问题</p><p>鹏哥说一篇好的论文不能缺少创新度,纯堆trick是不行的<br>我同意,但是,究竟哪些是创新点那些算trick呢</p><p>我做的六七个上分的大工作里排除已有的工作<br>私以为我的工作里并没有啥trick<br>或许我对trick的理解不大一样吧</p><p>如果在不太严格的条件下具有一定解释性的trick算不算创新点呢<br>我觉得算的</p><p>我认为的trick都是一些magic的leak<br>这些leak对实际意义起不到任何用处,但他就是能上分.<br>一些不具备通用性的调参技巧也算trick</p><p>现在大部分论文都靠搜索模型再加后向解释水了一个又一个会议<br>很难看到除模型之外的地方</p><p>2017 VQA Challenge 冠军的比赛报告是我比较喜欢的一篇文章<br><a href="https://arxiv.org/pdf/1708.02711.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.02711.pdf</a></p><p>除模型之外我个人认为的创新点有</p><ol><li>涉及到了zero-shot思想的初始化</li><li>检测模型做 hard attention</li><li>先进的采样算法<br>还有一些就是模型中的不具备通用性的trick了<br>例如特定情况下适用的激活函数</li></ol><p>聊到最后<br>其实我就是认为不管你学不学习,只要不作弊考到好分数都是好学生</p><p>trick具备通用性且后向解释能解释的通就好<br>通用性是前提,解释性可以稍稍靠后</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一直在搞video qa&lt;br&gt;这是一个比较新的方向,论文总共找到不到三篇,天池的视频问答比赛似乎也是全球第一届&lt;/p&gt;
&lt;p&gt;大家都是新手,一起开荒&lt;br&gt;前天和鹏哥聊着聊着看他有啥想法 .&lt;br&gt;最后聊到trick和创新度的问题&lt;/p&gt;
&lt;p&gt;鹏哥说一篇好的论文不能
      
    
    </summary>
    
      <category term="其他" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="其他" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>Home Credit</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/08/31/Home/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/08/31/Home/</id>
    <published>2018-08-31T08:46:51.000Z</published>
    <updated>2018-09-12T03:37:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>说起来这个比赛打起来体验极差…<br>线上线下琢磨不透…<br>时间上花了两周,最终结果single model cv 0.8004</p><p>开始三天在那抄抄dissussion和kernel里的代码拿到了cv 0.7963<br>后来觉得这样不行<br>后来花两天重构了下了代码,优化并行了特征生成部分<br>开始逐文件批量生成特征,再进行筛选 (毕竟一个变量名都看不懂)<br>接着效率大大提高,分数逐渐上升到8004<br>但是在这途中lb一直很差只有和cv差不多的分数<br>然后就丢了干别的比赛去了,打的烦啊,工作无聊机械,lb还上不去</p><p>最后一天把自己的oof和submit丢给队友就跑了<br>队友提交的最终cv是0.801吧</p><p>第八名的single model cv是0.799  集成804的样子<br>早知道最后一天随便跑跑了几个模型搞点儿差异性出来了…<br>小可惜,不过投入也不算多,不是特别心疼吧</p><p>这个比赛看下来搞特征的思路大多不是特别稀奇<br>强特的话批量找慢慢找都可以找出来的吧<br>主要是筛选工作和批量搜索的时候缩小搜索范围</p><p>我做的主要有</p><blockquote></blockquote><ol><li>时间差分特征  x<sub>t</sub> - x<sub>t-1</sub></li><li>趋势特征     过去一段时间内某变量的随时间变化趋势(即斜率)</li><li>趋势预测特征   用2所得的斜率预测当前值的预测值</li><li>一条record缺失值个数</li><li>特征分bin,计算各bin中的样本,数作为特征</li><li>一些条件概率,共现概率特征</li><li>target encode</li><li>批量搜索单位具有实际意义的特征,例如 钱/钱 , 钱/次数 , 次数/次数 , 次数 - 次数, 时间-时间<br>这么做的主要目的是减小搜索空间,毕竟几百个特征两两组合筛不过来呀</li></ol><p>我的收益几乎都来8 …<br>后来工作没干完就跑了,恩…</p><p>还有一些其他的思路下面再讲</p><p>筛选主要是这样的:</p><blockquote></blockquote><p>我首先划定了top550收益的特征,因为划到550我的cv上升了<br>然后每次批量生成几百个特征的时候留下gain落在top300的特征<br>注意事项: 必须要尽量缩小特征的生成量,因为特征生成的太多,收益均摊到各个特征上,导致大部分特征收益都很接近<br>    这时候不利于筛选</p><p>集成上虽然没做,但简单简述dissussion的magic集成思路<br>即用rank score进行集成<br>如果了解auc的话这个集成方式其实很容易想到</p><ol><li>因为AUC是排序指标<br>这种集成方式还有一个优势</li><li>因为不同的loss输出的预测值在[0,1]的方差不同,密度不同,期望也不同<br>直接加权可能问题就很大了<br>一种方式是把他们直接minmax缩放到[0,1],但这个没解决密度问题<br>另一个很简单的思路就是把预测值投射到rank上</li></ol><p>其他人的特征思路<br>对样本聚类500,然后用这个类别进行target encode<br>(话说聚类这东西我一直玩儿不溜,这东西到底能不能用自己每次都是试了才知道,所以要做都会堆到后期,没一点经验性的参考)<br>因为ext_1缺失值特别多,但他是重要特征,所以建立模型预测该特征作为新特征<br>这个带来收益也比较大</p><p>整个比赛里貌似只有第二名的方式是个亮点<br>他发现了样本以时间序排序,他用一些时间差分的特征如出生日期-上班日期等等特征得到了user_id<br>然后根据user_id和历史上下文信息对target进行编码得到了很大的提高<br>见<a href="https://www.kaggle.com/titericz/visualizing-user-ids" target="_blank" rel="noopener">https://www.kaggle.com/titericz/visualizing-user-ids</a></p><p>emmmmm  就这样吧</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说起来这个比赛打起来体验极差…&lt;br&gt;线上线下琢磨不透…&lt;br&gt;时间上花了两周,最终结果single model cv 0.8004&lt;/p&gt;
&lt;p&gt;开始三天在那抄抄dissussion和kernel里的代码拿到了cv 0.7963&lt;br&gt;后来觉得这样不行&lt;br&gt;后来花两天重
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="比赛总结" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>2018CVPR之Visual Question Answering with Memory-Augmented Networks阅读笔记</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/08/14/vqa-man/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/08/14/vqa-man/</id>
    <published>2018-08-14T14:30:18.000Z</published>
    <updated>2018-10-21T08:24:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>需求所致简单入门一下vqa</p><h3 id="Sequential-Co-Attention"><a href="#Sequential-Co-Attention" class="headerlink" title="Sequential Co-Attention"></a>Sequential Co-Attention</h3><p>如图所示<br>这是16年11月份一个序列协同注意力结构</p><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/co_attention.jpg" alt="这里写图片描述"> </p><p>直接放公式吧</p><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/e1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/q1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/v1.jpg" alt="这里写图片描述"></p><p>思路比较简单<br>{q<sub>t</sub>} question(word sequence)<br>{v<sub>n</sub>} 是通过一个backbone后再reshape的”区域”序列<br>如vgg16  -&gt;  16<em>16</em>512   -&gt;  256*512  每个像素代表一个区域</p><p>然后对图片的每个区域以及question的每个时间步做相关性计算<br>获得注意力权重加权….</p><h3 id="Memory-Augmented-Network"><a href="#Memory-Augmented-Network" class="headerlink" title="Memory Augmented Network"></a>Memory Augmented Network</h3><p>这里使用了Memory Network<br>是本文的创新点貌似</p><p>用(x<sub>t</sub>,y<sub>t</sub>)代表一个样本<br>t表示样本喂给模型的顺序</p><p>以LSTM 作为Memory Net的controller<br>h<sub>t</sub> = LSTM(x<sub>t</sub> , h<sub>t-1</sub>)<br>每次基于内容寻址<br>将h<sub>t</sub>与所有记忆单元计算相似度<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/d1.jpg" alt="这里写图片描述"><br>然后用softmax规范化相似度作为权重<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/w1.jpg" alt="这里写图片描述"><br>加权获得历史信息<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/ave1.jpg" alt="这里写图片描述"><br>与h<sub>t</sub>拼接放入分类网络</p><p>更新历史信息单元<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/08/14/vqa-man/mem.jpg" alt="这里写图片描述"><br>结束</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>作为一个放入VQA的小萌新<br>刷了几篇感觉好像比我想象中的要easy很多</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;需求所致简单入门一下vqa&lt;/p&gt;
&lt;h3 id=&quot;Sequential-Co-Attention&quot;&gt;&lt;a href=&quot;#Sequential-Co-Attention&quot; class=&quot;headerlink&quot; title=&quot;Sequential Co-Attention&quot;&gt;
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="QA" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>数据增广思考</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/07/29/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF%E6%80%9D%E8%80%83/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/07/29/数据增广思考/</id>
    <published>2018-07-29T08:23:18.000Z</published>
    <updated>2018-07-30T04:05:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>魔镜杯中第二名的数据增广使用了mixup<br>但词向量空间是否满足和图像一致的插值的性质值得深思<br>下面是自己的一些收货</p><p>从答辩情况来看,数据增广成功的只有三个人<br>第一第二以及yin叔</p><p>首先是样本生成方式<br>第一没有明说他的方式<br>第二介绍了mixup<br>yin叔则直接利用了相似问题对的稀疏性,断言生成的样本不匹配</p><p>yin叔的方式确有理论保证性,<br>随便估计一下如果官方给的78w个问题中共有10000类<br>以这个计算约两万个样本中只有一个正例 …<br>可见生成的样本label噪声很小</p><p>而mixup不敢笃定<br>图像的数据增广里mixup虽不易证明<br>但是其生成的样本结果肉眼可得到其label也是线性组合</p><p>词向量空间是否满足此性质不知<br>第二的大致思路是<br>q1,q2相似,q2,q3不相似<br>则q1+q2与q3仍不相似<br>似乎没有生成正样本<br>这里难以确定到底是正样本的稀疏性带来的收益还的确是mixup<br>不过不能再提交了,我猜是稀疏性…<br>不过第二好像和yin叔吐槽没啥效果 (所以说mixup并没有成功?到底有无收益?做成的变成2个人?)<br>不过看过来第二的确似乎是堆了多种模型给他带来的收益<br>从我们一直使用一种模型来看,最后一天使用了两个结构比较相似的模型却带来了巨大的收益<br>模型结构差异带来的收益可能更为显著</p><p>其实以上都不是重点<br>我也做了很多数据增强<br>yin叔的那种方式我也做了<br>都会过拟合…</p><p>但还有一个值得注意的地方<br>前排和yin叔数据增强的方式都是先用原有数据集训练<br>再从生成的数据集上进行微调</p><p>而我要么是放入训练集一起训练<br>要么是先用生成的样本获得权重…</p><p>反过来会成功的原因有待深究…</p><p>这次比赛和yin叔探讨中还获得了一个很有价值的东西<br>我使用prob label过拟合的原因似乎不知道,线下涨两个百分点,线上掉三个千分点<br>但是yin叔在一个预估用户评分的任务中给label加了高斯噪声成功了</p><p>他的任务可以很清楚的看到label本身噪声很大<br>给label加高斯噪声并无多大影响,反而能控制过拟合</p><p>数据增强的几个trick论文一览</p><p>label加噪的论文在此,效果较为显著<br>DisturbLabel<br><a href="https://arxiv.org/pdf/1605.00055v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1605.00055v1.pdf</a><br>label加噪也可以认为是一种数据增广的方式</p><p>MixUp论文<br><a href="https://arxiv.org/pdf/1710.09412.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1710.09412.pdf</a></p><p>数据增强之CutOut<br><a href="https://arxiv.org/pdf/1708.04552.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.04552.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;魔镜杯中第二名的数据增广使用了mixup&lt;br&gt;但词向量空间是否满足和图像一致的插值的性质值得深思&lt;br&gt;下面是自己的一些收货&lt;/p&gt;
&lt;p&gt;从答辩情况来看,数据增广成功的只有三个人&lt;br&gt;第一第二以及yin叔&lt;/p&gt;
&lt;p&gt;首先是样本生成方式&lt;br&gt;第一没有明说他的方式&lt;
      
    
    </summary>
    
      <category term="ML&amp;DL" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/ML-DL/"/>
    
    
      <category term="ML&amp;DL" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/ML-DL/"/>
    
  </entry>
  
  <entry>
    <title>魔镜杯比赛答辩PPT</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/07/25/%E9%AD%94%E9%95%9C%E6%9D%AF%E6%AF%94%E8%B5%9B%E7%AD%94%E8%BE%A9PPT/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/07/25/魔镜杯比赛答辩PPT/</id>
    <published>2018-07-25T11:36:38.000Z</published>
    <updated>2018-07-25T12:05:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>最终排名rank 6 </p><p>自己团队的解决方案<br>此次比赛主要目的用于促进团队的感情…</p><p>答辩情况来看,我们的集成做的太烂了…<br>一直只有一种模型,最后一天才反应到吃亏搞出两种很相似的上了俩千分点<br>靠着半监督的优势撑了大半个比赛流程</p><p>单模型情况(似乎是最高的?)<br>纯模型 0.153-0.154<br>结构特征 0.151-0.152<br>半监督(伪标签) 0.1475</p><p>PPT里藏了部分细节和思考<br>昨天中午和yin叔讨论中发现一些细节,发现了数据增强做成的技巧,发现了概率标签的可能适用场景,虽然仍然不知道他为啥过拟合<br>现在看来数据增强做成功的有三组队伍,冠军队,亚军队,yin叔<br>yin叔只用一种模型不用结构特征到这个分数还是很佩服的<br>小幸运组的tfidf加权词向量有待尝试</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0001.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0002.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0003.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0004.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0005.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0006.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0007.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0008.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0009.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0010.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0011.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0012.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0013.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0014.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0015.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0016.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0017.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0018.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0019.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0020.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/25/魔镜杯比赛答辩PPT/0021.jpg" alt="这里写图片描述"> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最终排名rank 6 &lt;/p&gt;
&lt;p&gt;自己团队的解决方案&lt;br&gt;此次比赛主要目的用于促进团队的感情…&lt;/p&gt;
&lt;p&gt;答辩情况来看,我们的集成做的太烂了…&lt;br&gt;一直只有一种模型,最后一天才反应到吃亏搞出两种很相似的上了俩千分点&lt;br&gt;靠着半监督的优势撑了大半个比赛流程&lt;/p
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="比赛总结" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Avito Demand Prediction Challenge比赛总结</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/07/07/Avito/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/07/07/Avito/</id>
    <published>2018-07-07T13:24:43.000Z</published>
    <updated>2018-07-12T13:30:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>这个比赛我是快结束的时候参加了,想顺手捞一波名次<br>他对我最大的挑战就是试错成本太高太高了</p><p>树模型交叉验证五折要5-7天 = =<br>图片70G<br>各种特征生成也有几十个G = = </p><p>我总共打了15天<br>写代码占4天,接下来搞别的事情去了,代码挂着跑了11天<br>最终拿了top6%的成绩,这种情况下拿到这个成绩个人还是很满意的</p><p>因为时间短不说,调试周期还长(占我打的时间的1/3~1/2了)<br>不过现在看下来他们的大部分trick我都做到了,就是没时间搞细</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="文本特征"><a href="#文本特征" class="headerlink" title="文本特征"></a>文本特征</h3><blockquote><p>自己的做法</p><ol><li>tfidf+char/word(稀疏矩阵存下来,几万维全部丢给树,svd效果会变差,大部分分数靠的是他)</li><li>地理位置信息用google api获取经纬度(没啥效果)</li></ol></blockquote><p>其他有价值的做法</p><ol><li>用fastText的词向量替代tfidf矩阵</li><li>文本的一些统计特征，词数，字数等等</li></ol><h3 id="图像特征"><a href="#图像特征" class="headerlink" title="图像特征"></a>图像特征</h3><blockquote><p>自己的做法</p><ol><li>图片的一些属性特征(写了没跑,因为具言效果不佳)</li><li>vgg的卷积结束后global pool成向量svd分解(效果不佳),其实这里可能丢失语义信息了,或许从全连接那里抽比较好(没时间了)</li></ol></blockquote><p>其他有价值的做法</p><ol><li>和我的2差不多类似换了基础网络resnet152，resnet34</li><li>还有一些图像的传统统计量亮度暗度（效果都不是很明显）</li><li>冠军用out-of-preditcion来微调resnet34 效果比较显著</li><li>预训练模型对图像进行分类作为特征（其实这个特征已经有了，所以比较冗余，不是很明显）</li></ol><h3 id="categorical-amp-numerical"><a href="#categorical-amp-numerical" class="headerlink" title="categorical &amp; numerical"></a>categorical &amp; numerical</h3><p>user_id和item_id 几乎一个样本一个id了,表示学习都没法了<br>而且user_id训练集测试集交集只有6%,item_id只有0%</p><blockquote><p>自己的做法<br>1  &emsp; 用user_type和city的组合编码近似代表user<br>&emsp;&emsp;用category,image_top_1等近似代表item<br>&emsp;&emsp;获取邻接矩阵进行svd分解(分数提升)<br>2  &emsp; category批量生成共现概率，条件概率<br>3  &emsp; category与连续组合计算条件统计量<br>4  &emsp; 连续连续组合+-*/<br>5  &emsp; target encode(用category和label组合计算label的统计量）</p></blockquote><p>其他有价值的做法</p><ol><li>冠军把user_id用起来了，虽然user_id在训练集测试集中交集只有6%，但他交叉验证时将user_id分的和测试集差不多比例</li></ol><blockquote></blockquote><p>category之间计算共现概率,条件概率(条件熵,信息熵等没算)<br>数值和类别之间计算条件统计量(mean,min,max,std)等<br>交叉组合四五个特征</p><blockquote><p>虽说我的特征大都是批量生成<br>但还是选择性的批量生成，否则维度太高了<br>其中起主要效果的是price的条件统计量，以及price条件统计量的标准化</p></blockquote><h2 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h2><blockquote></blockquote><ol><li>spatialDropout1D放embedding后超级棒，我最近发现先bn再sdp效果极佳</li><li>图像部分迁移学习固定权重</li><li>树模型与nn都使用交叉熵效果极佳，树模型是xentropy</li></ol><blockquote><p>值得一提的是好多文本比赛感觉attention很不work，我猜可能大部分都是短文本的缘故吧</p></blockquote><h2 id="集成"><a href="#集成" class="headerlink" title="集成"></a>集成</h2><p>没做 = =<br>跑出了俩模型,第一个太烂丢掉了<br>实际有效的模型只跑出了一个，只能随便拉个kerne的平均一下</p><blockquote><p>其他人的做法<br>几组较优超参集成<br>疯狂stacking…</p></blockquote><h2 id="最后感受"><a href="#最后感受" class="headerlink" title="最后感受"></a>最后感受</h2><p>大家做的方法其实都大差不差<br>都是一些细节拉大了分数<br>我感觉我吃的最大的亏是时间<br>在细节上我用的是回归，别人用的是交叉熵</p><p>和鹏哥感受一样比赛真的是一件投入产出比很低 = =<br>从头打到尾套路基本都会了的话<br>不出意外个人感觉top3%,top2%不是意见特别难的事情<br>但是很耗精力，比赛周期一般也太长了。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个比赛我是快结束的时候参加了,想顺手捞一波名次&lt;br&gt;他对我最大的挑战就是试错成本太高太高了&lt;/p&gt;
&lt;p&gt;树模型交叉验证五折要5-7天 = =&lt;br&gt;图片70G&lt;br&gt;各种特征生成也有几十个G = = &lt;/p&gt;
&lt;p&gt;我总共打了15天&lt;br&gt;写代码占4天,接下来搞别的事
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="比赛总结" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>GRL简述</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/07/01/GRL%E7%AE%80%E8%BF%B0/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/07/01/GRL简述/</id>
    <published>2018-07-01T04:53:52.000Z</published>
    <updated>2018-10-13T07:58:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>nlp讨论班需要简要入门一下GRL<br>在此做个简短的介绍<br>这东西对大家打比赛也是很有用的<br>而且个人觉得这个方向也是比较有趣的</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_01.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_02.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_03.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_04.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_05.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_06.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_07.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_08.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_09.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/07/01/GRL简述/0_10.jpg" alt="这里写图片描述"> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;nlp讨论班需要简要入门一下GRL&lt;br&gt;在此做个简短的介绍&lt;br&gt;这东西对大家打比赛也是很有用的&lt;br&gt;而且个人觉得这个方向也是比较有趣的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qr
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="representation learning" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/representation-learning/"/>
    
  </entry>
  
  <entry>
    <title>attention</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/06/29/attention/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/06/29/attention/</id>
    <published>2018-06-29T13:33:51.000Z</published>
    <updated>2018-06-29T13:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>nlp讨论班刚开始没多久<br>得从基础唠起<br>遂做了个简短的注意力ppt<br>做个简单的介绍</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0001.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0002.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0003.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0004.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0005.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0006.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0007.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/06/29/attention/0008.jpg" alt="这里写图片描述"> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;nlp讨论班刚开始没多久&lt;br&gt;得从基础唠起&lt;br&gt;遂做了个简短的注意力ppt&lt;br&gt;做个简单的介绍&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qrfaction/qrfaction.gi
      
    
    </summary>
    
      <category term="ML&amp;DL" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/ML-DL/"/>
    
    
      <category term="attention" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/attention/"/>
    
  </entry>
  
</feed>
