<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>qrfaction的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/qrfaction/qrfaction.github.io/"/>
  <updated>2019-02-28T11:57:52.632Z</updated>
  <id>https://github.com/qrfaction/qrfaction.github.io/</id>
  
  <author>
    <name>qrfaction</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>瑞金知识图谱比赛总结</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2019/02/28/%E7%91%9E%E9%87%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2019/02/28/瑞金知识图谱/</id>
    <published>2019-02-28T11:45:29.000Z</published>
    <updated>2019-02-28T11:57:52.632Z</updated>
    
    <content type="html"><![CDATA[<p>这个小破赛到昨天才答辩，吐槽一波(恩，好像没隔壁oppo惨，hh)</p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>涨分的应该都说了，但有些具体多少实在忘掉了<br>答辩队友没准备充分，部分答错加答漏，导致被翻盘少了两万，此处拉出来鞭尸<br>复赛2nd,答辩3rd</p><h3 id="初赛"><a href="#初赛" class="headerlink" title="初赛"></a>初赛</h3><p>命名体识别问题，采用序列标注方式<br>数据特点：数据集脏，句子不易切分，句子长度方差较大，长句严重影响训练调试速度</p><p>我们初期着重点一直在如何加速调试<br>我们初期直接采用了sota：LSTM+CRF(不考虑外部工具及模型)<br>然而这个模型实在训练太慢了<br>0) 预处理一种是按规则切分和按行切分，按行略差3个千<br>1) 改模型LSTM为CuDNNGRU/CuDNNLSTM<br>&emsp;&emsp;速度*5～10具体多少忘了，分数下降10个点(CuDNNGRU/CuDNNLSTM不支持mask zero)<br>2) 由于CuDNNGRU/CuDNNLSTM不支持mask zero<br>&emsp;&emsp;实现伪mask zero(解决循环单元前传，反传无法解决)，上涨8个点<br>3) 解决句子边缘识别太差<br>&emsp;&emsp;sample = [sentence1,sentence2,sentence3]<br>&emsp;&emsp;y = [y1,y2,y3]<br>&emsp;&emsp;输入输出改成窗口为三同时输入三句话的方式，最后结果只取y2<br>&emsp;&emsp;因为中间句sentence2没有边缘信息不足问题<br>&emsp;&emsp;这个方式训练速度下降三倍，不用CuDNNGRU一般是跑不动的。。。<br>&emsp;&emsp;分数上涨2-4个点<br>4) 添加词性词边界特征将其embedding后与字向量拼接<br>&emsp;&emsp;上涨1-2个点<br>5) 将GRU容量压到最小后开始逐层调Spatial Dropout<br>6) 差分学习率，embedding层学习率小十倍，上涨1个点左右<br>7) 按行切分和按规则切分集成分数上涨0.2~0.3个点</p><p><er><br>以上是初赛的解决方案<br>其实线下再加一层CuDNNGRU还可以涨3-4个百分点<br>由于之前实验未控制好变量，得到错误结论故一直没采用<br>CuDNNGRU两层后速度彻底和两层GRU的速度拉开来了超多倍</er></p><p>初赛主要赢在让模型训练速度大幅度加倍的同时只是略降分数<br>使得我们可以采用很暴力的上分方式</p><p><er></er></p><h3 id="复赛"><a href="#复赛" class="headerlink" title="复赛"></a>复赛</h3><p>实体对关系分类问题<br>数据特点：<br>1) 已知实体对可能关系类别，但未知是否成立<br>2) 数据集标签存在一定噪声(存在关系的实体对相隔太远)</p><p>核心思路<br>0) 实体对两两组合，并将覆盖他们的句子切出进行分类，其中过滤长度大于200的句子<br>1) 覆盖实体对的句子在两端各取一定长度的短句丰富上下文信息</p><blockquote></blockquote><p>if end - begin &lt;= 100:<br>    padding = 50     # window是在两边补多少词<br>elif end - begin &lt;= 140:<br>    padding = 30<br>else:<br>    padding = 0</p><p>2) 特征工程<br>&emsp;&emsp;1.实体对距离特征：这个特征原本没有用，因为句子的补0数量可以让模型捕捉到长度信息<br>&emsp;&emsp;但是1)中的方法使得这个信息丢失，让模型不易捕捉<br>&emsp;&emsp;2.词性特征+position embedding(每个字到实体1,2的距离)，与字向量拼接<br>&emsp;&emsp;3.实体segment特征，实体1标0.1，实体2为-0.1，其他为0，与字向量相加<br>&emsp;&emsp;&emsp;&emsp;选0.1主要考虑到默认初始化的方差大小，故选择一个较小的值<br>&emsp;&emsp;&emsp;&emsp;这个特征一方面是标示出实体对 yu的差异与其他字的差异<br>&emsp;&emsp;&emsp;&emsp;另一方面是为了后面利用他获得实体1,2的特征向量<br>&emsp;&emsp;4.邻近实体对存在关系的概率特征(这个好像没什么人发现)<br>&emsp;&emsp;&emsp;&emsp;example:<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;中国2型糖尿病防治指南(2010)在LSM和二甲双胍基础上HBA1C,≥7.O%:<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;新诊断有明显体重减轻或其他严重高血糖状的患者<br>&emsp;&emsp;&emsp;&emsp;我们观察到 (2型糖尿病,LSM) 是否存在关系,还与 (2型糖尿病,二甲双胍) ，(2型糖尿病,HBA1C)有关</p><p>&emsp;&emsp;&emsp;&emsp;其实这个特征真不是看数据得到的。。<br>&emsp;&emsp;&emsp;&emsp;主要是开始分fold发现不按文档分而按句子分会严重leak验证集，线上线下对不上<br>&emsp;&emsp;&emsp;&emsp;既然leak了验证集，如果能利用好还是能劣势转优势的<br>&emsp;&emsp;&emsp;&emsp;查找leak的原因后，观察发现文章中某些实体对强相关。<br>&emsp;&emsp;&emsp;&emsp;然后用初版模型对实体对打分，将邻近实体对的分数作为特征上了不到一个百<br>&emsp;&emsp;&emsp;&emsp;(前面特征用太多了，快上限了)<br>3) 模型<br>&emsp;&emsp;两层GRU + SpatialDropout + attention pooling + dropout + fc + 差分学习率<br>4) attention (1~2个点)<br>&emsp;&emsp;使用之前的segment标示出的实体位置，在输入的text中取出对应字向量<br>&emsp;&emsp;并将其平均处理，再通过权重共享的全连接层将文本和字映射到同一个空间<br>&emsp;&emsp;使用e2-(e1+text)作为相似度计算的方式(参考知识图谱的embedding算法)<br>&emsp;&emsp;e2-(e1+text)越小权重越高<br>&emsp;&emsp;然后用获得的attention pooling进行加权平均<br>5) prior mask实现独立分类器<br>&emsp;&emsp;我们的输出非11个单元，也非1个单元的二分类<br>&emsp;&emsp;而是输出了10个单元的二分类(即sigmoid激活，不是softmax)<br>&emsp;&emsp;由于我们之前知道了实体对可能的类别<br>&emsp;&emsp;故可以生成类似如下的向量<br>&emsp;&emsp;prior mask = [0,0,0,0,0,1,0,0,0,0]<br>&emsp;&emsp;而模型输出output = [0.1,0.9,0.5,0.4,0.3,0.2,0.1,0.5,0.6,0.7]<br>&emsp;&emsp;output = output * prior mask = [0,0,0,0,0,0.2,0,0,0,0]<br>&emsp;&emsp;然后用bce计算loss回传梯度<br>&emsp;&emsp;这个方式结合了11分类与2分类的好处<br>&emsp;&emsp;11分类会有类间干扰，2分类会导致所有类别用同一个分类器权重<br>6) 知识蒸馏二次训练3个千<br>7) 替换GRU为LSTM训练集成，3个千</p><p>最后<br>比赛比较懒，数据清洗方面一直懒得下功夫<br>模型也都是用之前的经验，懒得调试<br>loss也懒得动<br>其他队正好和我们队互补，可以参考一下他们的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个小破赛到昨天才答辩，吐槽一波(恩，好像没隔壁oppo惨，hh)&lt;/p&gt;
&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;涨分的应该都说了，但有些具体多少实在忘掉了&lt;br&gt;答辩队友
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="知识图谱" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>Alibaba Cloud German AI Challenge 比赛总结</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2019/02/14/german-solution/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2019/02/14/german-solution/</id>
    <published>2019-02-14T02:08:56.000Z</published>
    <updated>2019-02-15T08:23:30.861Z</updated>
    
    <content type="html"><![CDATA[<p>赛季1：主要是hadxu和haowei在干活儿<br>赛季2：主要是我和haowei在干活儿</p><p>源码链接：<a href="https://github.com/lhwcv/cloud_german_rank10">https://github.com/lhwcv/cloud_german_rank10</a></p><h2 id="赛题分析"><a href="#赛题分析" class="headerlink" title="赛题分析"></a>赛题分析</h2><p>题目数据来自卫星数据有18通道<br>各通道数据区间相差较大<br>比较麻烦的是验证集与测试集同分布，而训练集非同分布<br>而且训练集，验证集和测试集疑似存在很多伪重图片，造成线下cv和线上难以对上</p><p>个人一直很烦这种问题，大部分找不到很好的解决方案，只能采取一些维稳策略<br>毕竟只有良好的验证集才能帮助正确评估模型快速迭代优化方案<br>各位看客有比较好的解决方案望告知</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>讲的很细，因为没啥东西，算是个抛砖引玉<br>个人感觉基础分类任务创新也是蛮难得，基本上看到都是loss啥变变</p><ol><li>可见光通道所在的卫星数据进行亮度对比度数据增广</li><li>所有数据进行flip+rotate</li><li>数据z-score规范化使各通道范数相近(据队友实验数据好像归一化会严重掉分，不过我没这习惯hh)</li><li>卫星数据reflect放大，一直测试到160*160，因为春节大家都比较懒只测到这么大了，一直有涨</li><li>学习率三次衰减，再衰减线下虽还未过拟合，线上分数已经开始掉了</li><li>label smooth  -&gt;  label = (1-alpha)<em>label+alpha</em>(1-label)/16</li><li>batch level ohem </li><li>Nadam+pytorch</li><li>deep backbone + light head</li><li>使用初版模型获得训练数据的fc特征，再聚类聚5类分fold（约几个千）</li><li>半监督伪标签，四份测试集取较高分数段滤掉部分低质量样本加入训练集训练</li><li>TTA × 8</li><li>112,128,144,160大小图像差异集成，xcep + IncepResnetv2 + seresnext50 + resnet50集成</li></ol><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>恩，方案简简单单普普通通<br>模型也还行，春节期间比较忙，最后临时抱佛脚性质跑了四个模型<br>等着前排答辩完开个方案</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;赛季1：主要是hadxu和haowei在干活儿&lt;br&gt;赛季2：主要是我和haowei在干活儿&lt;/p&gt;
&lt;p&gt;源码链接：&lt;a href=&quot;https://github.com/lhwcv/cloud_german_rank10&quot;&gt;https://github.com/lhwc
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="图像分类" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Gradient Harmonizing Mechanism简读</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2019/01/31/GHM/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2019/01/31/GHM/</id>
    <published>2019-01-31T13:47:31.000Z</published>
    <updated>2019-01-31T14:38:23.573Z</updated>
    
    <content type="html"><![CDATA[<p>文章链接 <a href="https://arxiv.org/pdf/1811.05181.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.05181.pdf</a></p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>通过数据集的梯度分布去调节训练时权重<br>旨在降低label噪声影响和对hard example更好的学习</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/31/GHM/ghmc.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/31/GHM/beta.png" alt="这里写图片描述"></p></blockquote><h3 id="Gradient-Density"><a href="#Gradient-Density" class="headerlink" title="Gradient Density"></a>Gradient Density</h3><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/31/GHM/gn.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/31/GHM/gd.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/31/GHM/g.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/31/GHM/g_d.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/31/GHM/gd_detail.png" alt="这里写图片描述"><br>意思就是密度越大的样本权重越低<br>密度大且loss大的样本认为是outlier</p></blockquote><h3 id="Unit-Region-Approximation"><a href="#Unit-Region-Approximation" class="headerlink" title="Unit Region Approximation"></a>Unit Region Approximation</h3><p>将梯度分区间段统计计算梯度密度<br>很简单的一个近似估计来降低算法时间复杂度</p><h3 id="Exponential-Moving-Average"><a href="#Exponential-Moving-Average" class="headerlink" title="Exponential Moving Average"></a>Exponential Moving Average</h3><p>梯度密度是根据一个mini-batch的样本统计所得<br>通常情况下batch不能很好反应整个数据集的分布情况<br>梯度密度变化十分不稳定<br>通过滑动平均让这个变化变得更加smooth</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;文章链接 &lt;a href=&quot;https://arxiv.org/pdf/1811.05181.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1811.05181.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;核
      
    
    </summary>
    
      <category term="ML&amp;DL" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/ML-DL/"/>
    
    
      <category term="ML&amp;DL" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/ML-DL/"/>
    
  </entry>
  
  <entry>
    <title>graph conv</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2019/01/13/gcn/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2019/01/13/gcn/</id>
    <published>2019-01-13T07:46:51.000Z</published>
    <updated>2019-01-13T07:51:47.846Z</updated>
    
    <content type="html"><![CDATA[<p>主要是将知乎上这个回答按自己的逻辑整理了一下<br><a href="https://www.zhihu.com/question/54504471/answer/332657604" target="_blank" rel="noopener">https://www.zhihu.com/question/54504471/answer/332657604</a></p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_2.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_3.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_4.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_5.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_6.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_7.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/13/gcn/0_8.jpg" alt="这里写图片描述"> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;主要是将知乎上这个回答按自己的逻辑整理了一下&lt;br&gt;&lt;a href=&quot;https://www.zhihu.com/question/54504471/answer/332657604&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.z
      
    
    </summary>
    
      <category term="GNN" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/GNN/"/>
    
    
      <category term="GCN" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/GCN/"/>
    
  </entry>
  
  <entry>
    <title>kaggle Human Protein Atlas 比赛总结</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2019/01/12/HPA/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2019/01/12/HPA/</id>
    <published>2019-01-12T13:10:39.000Z</published>
    <updated>2019-01-12T13:20:38.673Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个是要被队友借去改改发知乎上的<br>给别人写的不能像自己的凑合，所以这次写的总结会比以往的都正式很多。。<br>此次比赛和队友一起拿下了金牌，最终名次11/2172<br>原本是想着积分榜第一的巨佬<a href="https://www.kaggle.com/bestfitting" target="_blank" rel="noopener">bestfitting</a>公开方案后写个方案汇总的<br>但怕是等不到了。。。<br>这个比赛是对于believe your cv的死教条的一个很好的反面示例<br>那些不会依据客观事实变通只知道守着死教条的人都死得很惨！！！</p><h2 id="比赛介绍"><a href="#比赛介绍" class="headerlink" title="比赛介绍"></a>比赛介绍</h2><h3 id="任务介绍"><a href="#任务介绍" class="headerlink" title="任务介绍"></a>任务介绍</h3><p>本次比赛的问题是场景识别<br>定位的是蛋白质所处位置的识别,例如在细胞液，细胞核里之类的<br>依输出分所属多标签分类</p><h3 id="难点及数据介绍"><a href="#难点及数据介绍" class="headerlink" title="难点及数据介绍"></a>难点及数据介绍</h3><p>首先一张示例图如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/12/HPA/result.png" alt="这里写图片描述"> </p><p>1) 一个样本由四个”通道图”组成(分成四张存储，每个”通道图”单通道)，2048*2048大小，官方提供了插值缩小为512大小的数据<br>2) 外部数据70k(每个”通道图”都是rgb格式)<br>3) 图片伪重多，然后四个”通道图”，id相同但实例不同的情况也很多，如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2019/01/12/HPA/1541446267.jpg" alt="这里写图片描述"> </p><p>1)，2)对比我们发现了外部数据总共有4 <em> 3个通道，而官方数据只有4 </em> 1个<br>这便带来了第一个难点，如何处理外部数据<br>2)，3)最麻烦的是伪重问题，伪重造成验证集leak，不能制作一个好的验证集</p><p>外部数据处理后加入训练集，要制作一个好的验证集就得跨过两道坎</p><ol><li>外部数据无论咋处理总是和测试集分布有些不一样的</li><li>训练集里伪重过多，如何保证验证集不leak</li></ol><p>这个问题貌似只有bestfitting解决了，但是他还没公开方案。。。<br>1)2)3)共同造成的线下val的不可靠，这时候线上比线下可靠，但仍需注意不要过拟合线上</p><h3 id="our-solution"><a href="#our-solution" class="headerlink" title="our solution"></a>our solution</h3><h4 id="model-config1"><a href="#model-config1" class="headerlink" title="model config1"></a>model config1</h4><ol><li>数据增广(亮度对比度调整，crop_resize,flip,other)<br>我们总共用了12倍的TTA<br>亮度对比度主要用于让模型对外部数据和官方数据差异的鲁棒性更好<br>crop_resize对于显微镜下实例大小方差巨大的情况，带来了较好的线下收益<br>受限于调试成本，为了不避免数据增广带来的分布偏移问题，于是只选择性的尝试了上述几种，不过也一蒙及中。</li><li>Models<br>res18 (batchsize=64)<br>res34 (batchsize=32)<br>bninception (batchsize=32)<br>inceptionv3 (batchsize=32)<br>xception (batchsize=24, P40-24G)<br>se-resnext50(batchsize=24, P40-24G)<br>其中模型对batchsize大小十份敏感，正确来说是模型对参与batchnorm计算的样本数十分敏感，需用sync-bn或者P40(24G)显存实现</li><li>lr schedule<br>Nadam优化器<br>阶段性学习率衰减<br>差分学习率（其中初层用于转换通道的卷积和最后的model head同学习率，中间的backbone学习率小一倍）</li></ol><h4 id="model-config2"><a href="#model-config2" class="headerlink" title="model config2"></a>model config2</h4><p>这个方案来自后来加入的队友<a href="https://www.kaggle.com/shisususu" target="_blank" rel="noopener">shisu</a><br>只简述一下差异的部分</p><ol><li>lr schedule<br>SGD 优化器<br>cosine lr </li><li>model<br>res18/34 (96batchsize + softf1_loss + float16 + 过采样)<br>巨型图片1024*1024上训练出来的(我们只有他成功下载了这部分数据，帮不了他了)</li></ol><h4 id="postprocess"><a href="#postprocess" class="headerlink" title="postprocess"></a>postprocess</h4><p>基本是利用了伪重的性质<br>将测试集中的相似图片的预测进行共享，其中预测选择图片质量较高的那张，约0.002的收益</p><h4 id="ensemble"><a href="#ensemble" class="headerlink" title="ensemble"></a>ensemble</h4><p>外部数据灰度化和外部数据取高范数的通道两种方案得到了两份数据<br>两份数据上训练出来的模型进行差异化集成<br>model config1 和 config2进行集成</p><h3 id="写到最后"><a href="#写到最后" class="headerlink" title="写到最后"></a>写到最后</h3><p>附上其他高质量的分享，个人感觉都不是很新颖<br><a href="https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77251" target="_blank" rel="noopener">8th place solution</a><br><a href="https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77269" target="_blank" rel="noopener">7th place solution</a><br><a href="https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77300" target="_blank" rel="noopener">4th place solution</a><br><a href="https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77320" target="_blank" rel="noopener">3th place solution</a><br><a href="https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77325" target="_blank" rel="noopener">12th place solution</a><br>看完这些，会看到有部分LSEP loss,GapNet，autoAugment等花哨的东西<br>但私以为仍不是重点<br>其中各种大小的图进行ensemble才是重点，特别是1024*1024 。。。<br>正确的数据增强和不要太低的线上</p><p>其中一个比较具有通用性的trick，可供大家参考<br><a href="https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/76665" target="_blank" rel="noopener">https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/76665</a><br>图片分成多个patch,将baseline模型对这些块输出分数<br>拿那些响应最高的patch重新训练一次模型<br>相当于一个hard attention</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这个是要被队友借去改改发知乎上的&lt;br&gt;给别人写的不能像自己的凑合，所以这次写的总结会比以往的都正式很多。。&lt;br&gt;此次比赛和队友一起拿下了
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="细粒度分类" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>18总结与未来展望</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2019/01/01/18%E5%B9%B4%E7%9A%84%E5%9B%9E%E9%A1%BE%E4%B8%8E19%E5%B9%B4%E7%9A%84%E5%B1%95%E6%9C%9B/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2019/01/01/18年的回顾与19年的展望/</id>
    <published>2019-01-01T10:20:18.000Z</published>
    <updated>2019-01-01T10:21:59.384Z</updated>
    
    <content type="html"><![CDATA[<p>2018整体感觉自己进步还是蛮大的<br>但是细究起来自己有一堆计划没有完成<br>下半年的时间利用率很低，2019需要加以调整</p><h3 id="2018年"><a href="#2018年" class="headerlink" title="2018年"></a>2018年</h3><h4 id="达成成就"><a href="#达成成就" class="headerlink" title="达成成就"></a>达成成就</h4><ol><li>攒下四五个拿得出手的比赛成绩了</li><li>gpu编程了解并完成node2vec加速，虽然感觉现在看起来很烂</li><li>github差不多满绿</li><li>nlp/cv纵向横向知识面拓宽，RL完成一本书的基础阅读量</li><li>抽象思考tabular data切入点并实践</li><li>博客50+</li><li>多次参与国内赛，基本认识了一下圈内人，拓展了一下交际圈</li><li>学生时代达成财富自由</li></ol><h4 id="遗憾"><a href="#遗憾" class="headerlink" title="遗憾"></a>遗憾</h4><p>kaggle大部分都只搞了一周两周，半步金牌，浪费了时间<br>下半年太过松懈，时间利用率远没上半年高<br>没能早点意识去收集比赛称号</p><h3 id="对2019年的期望"><a href="#对2019年的期望" class="headerlink" title="对2019年的期望"></a>对2019年的期望</h3><ol><li>达成grandmaster和天池数据科学家称号（如果能有数据大师超级棒啊）</li><li>一线互联网公司实习</li><li>高性能计算方向能深入下去（冒博啥时候写完书诶）</li><li>博客量100+</li><li>github继续差不多满绿</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2018整体感觉自己进步还是蛮大的&lt;br&gt;但是细究起来自己有一堆计划没有完成&lt;br&gt;下半年的时间利用率很低，2019需要加以调整&lt;/p&gt;
&lt;h3 id=&quot;2018年&quot;&gt;&lt;a href=&quot;#2018年&quot; class=&quot;headerlink&quot; title=&quot;2018年&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="生活" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="生活" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>pointer,copy,coverage机制简述</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/12/28/seq2seq-pointer%E6%9C%BA%E5%88%B6/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/12/28/seq2seq-pointer机制/</id>
    <published>2018-12-28T07:35:36.000Z</published>
    <updated>2018-12-28T07:49:45.723Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Pointer-Networks"><a href="#Pointer-Networks" class="headerlink" title="Pointer Networks"></a>Pointer Networks</h3><p><a href="https://arxiv.org/pdf/1506.03134.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.03134.pdf</a></p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/point1.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/point2.png" alt="这里写图片描述"><br>如上，其实就是直接把注意力权重的结果输出为最后结果了<br>即在原序列中抽样<br>旨在解决凸包和旅行商等类似问题</p></blockquote><h3 id="Copying-Mechanism"><a href="#Copying-Mechanism" class="headerlink" title="Copying Mechanism"></a>Copying Mechanism</h3><p><a href="https://arxiv.org/pdf/1603.06393.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06393.pdf</a></p><p>理解这个我们首先看个使用场景即如下</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/copy1.png" alt="这里写图片描述"> </p></blockquote><p>本文旨在解决oov和低频词版本<br>让模型学会适当从历史信息中copy文本</p><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/copymodel.png" alt="这里写图片描述"><br>模型即上图<br>首先看模型的输入输出<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/copy_o1.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/copy_o2.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/copy_o3.png" alt="这里写图片描述"><br>其中c<sub>t</sub>是当前状态对输入序列的注意力加权后的输出向量<br>M是输入序列的隐藏态h<sub>1</sub> to h<sub>t</sub><br>其中s<sub>t</sub> = f(y<sub>t-1</sub>,s<sub>t-1</sub>,c)是decoder的隐藏态<br>这里多加了一个输入，即从M中获得<br>如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/copy_s.png" alt="这里写图片描述"><br>将y<sub>t-1</sub>在输入中多次出现的词考虑进来</p><p>整体来讲模型给那些 不曾在字典中出现过但又在输入中出现过的词 一个输出口<br>让模型在没能很好学习rare word和oov的语义的情况下能具备一些复制能力</p><h3 id="Coverage-Mechanism"><a href="#Coverage-Mechanism" class="headerlink" title="Coverage Mechanism"></a>Coverage Mechanism</h3><p>Get To The Point: Summarization with Pointer-Generator Networks<br><a href="https://arxiv.org/pdf/1704.04368.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04368.pdf</a><br>总得来讲<br>首先维护了在历史状态下decoder时输出的attention权重<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/coverage1.png" alt="这里写图片描述"><br>将其作为特征输入，用于计算当前attention<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/coverage2.png" alt="这里写图片描述"><br>然后对于那些重复出现过高权重的词，给予适当惩罚<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/28/seq2seq-pointer机制/coverage3.png" alt="这里写图片描述"><br>目的应该是希望模型不要老出现重复词</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Pointer-Networks&quot;&gt;&lt;a href=&quot;#Pointer-Networks&quot; class=&quot;headerlink&quot; title=&quot;Pointer Networks&quot;&gt;&lt;/a&gt;Pointer Networks&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="seq2seq" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>SlowFastNet</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/12/18/slowfastnet/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/12/18/slowfastnet/</id>
    <published>2018-12-18T10:49:44.000Z</published>
    <updated>2018-12-21T12:11:41.913Z</updated>
    
    <content type="html"><![CDATA[<p>cv讨论班自己做的简短汇报<br>行为动作识别任务</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/18/slowfastnet/0_1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/18/slowfastnet/0_2.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/18/slowfastnet/0_3.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/18/slowfastnet/0_4.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/18/slowfastnet/0_5.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/18/slowfastnet/0_6.jpg" alt="这里写图片描述"> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;cv讨论班自己做的简短汇报&lt;br&gt;行为动作识别任务&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/18
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="行为动作识别" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E8%A1%8C%E4%B8%BA%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>2017CVPR RACNN</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/12/04/2017CVPR_RACNN/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/12/04/2017CVPR_RACNN/</id>
    <published>2018-12-04T00:32:42.000Z</published>
    <updated>2018-12-04T00:44:10.623Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章其实是篇比较简单的文章<br>但是其中有个梯度反传的细节很多博客都没纠清楚<br>文章里也没细讲</p><p>这篇文章主要就是利用弱监督定位去做细粒度分类<br>想法还是比较有意思的，上一次见到窗口定位的方式是在local attention</p><p>要弱监督定位需要把梯度传到坐标上，如果坐标进行取整则不能反传梯度<br>坐标的梯度更新是通过他设计的mask传回去的</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0001.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0002.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0003.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0004.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0005.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/12/04/2017CVPR_RACNN/0006.jpg" alt="这里写图片描述"> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章其实是篇比较简单的文章&lt;br&gt;但是其中有个梯度反传的细节很多博客都没纠清楚&lt;br&gt;文章里也没细讲&lt;/p&gt;
&lt;p&gt;这篇文章主要就是利用弱监督定位去做细粒度分类&lt;br&gt;想法还是比较有意思的，上一次见到窗口定位的方式是在local attention&lt;/p&gt;
&lt;p&gt;要弱监
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="细粒度分类" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>之江视频问答1st比赛总结</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/12/03/%E4%B9%8B%E6%B1%9F%E8%A7%86%E9%A2%91%E9%97%AE%E7%AD%941st%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/12/03/之江视频问答1st比赛总结/</id>
    <published>2018-12-03T05:04:49.000Z</published>
    <updated>2018-12-04T00:58:45.636Z</updated>
    
    <content type="html"><![CDATA[<p>由于PPT上都是图，直接发打字比较好</p><p>PPT那么多页其实也就几段字，哪有那么多东西讲</p><ol><li><p>mp4关键帧I帧抽取，抽取I帧至40帧，不足则在I帧之间补足40帧</p></li><li><p>2017vqa冠军的模型 faster rcnn<br> (1) 抽取36个物体，受限于IO效率，物体和帧之间以帧为重<br> (2) attribute特征，把输出bbox的属性词作为特征，一是无io压力，二是视频物体种类变化小，可依据该事实通过频率提高该特征信噪比<br> 无效模型 densecap : image caption + action location<br> 本想用它解决动作类问题，无奈句子不方便通过频率筛选，提高特征质量，以及他本身效果不算特别好<br> 外加句子关键词太少</p></li><li><p>prior mask<br> 各类question答案词表分布很不一致，如颜色类问题，数量类问题<br> 很容易想到希望对每一类问题针对性处理，可通过用一个prior mask实现，将该类问题可能回答出的词标1，否则标0，<br> sigmoid激活后与之相乘，避免常识性错误，以及免去类间干扰(question类别)<br> 如此实现了特征共享，但每类问题针对性处理，且可自由扩展类别数量<br> question类别用最易于区分的关键字作为类，如what color ,how many 以及or类问题的or两边的词</p><p> 很容易有下面的改进方法(我没去做,感兴趣试试)</p><p> (1) 以2中抽取的视频attribute作为视频关键信息，通过这个抽取视频先验词表改进prior mask<br> (2) 减小类内干扰，如深红，浅红，细粒度分类无论在nlp还是cv都是个较难的任务,<br>&emsp;&emsp;但是不用细粒度即可答对，干脆放弃细粒度分类<br>&emsp;&emsp;这个问题可以抽象为用尽可能少的答案覆盖该类问题所有答案</p></li><li><p>问答框架<br> 多问多答，可变长输入<br> 问题不足五个重复补足</p><ol><li>免去冗余的五次io</li><li>带来上下文信息</li><li><p>劣势是偏固定的采样方式使他分数若不使用上下文信息会劣于一般训练方式</p><p>改进方法：用0补，重复会让特征信号加强，然后loss梯度权重加倍。。。<br>用0补的话是常量无信息，输出时可配合mask把梯度屏蔽掉</p></li></ol></li><li><p>针对分镜问题的数据增强方案<br> 找俩视频，各取一半帧数，时间轴拼接<br> 问题的话随便取出5个<br> attribute各取频率最高的一半<br> 一问一答，label直接取逻辑加</p></li><li><p>其他数据增强<br> (1) 训练时抽来的40帧随机取16帧 (帧数取太少的话不会有效果的)<br> (2) 测试时隔帧各采16帧，做test aug</p></li><li><p>模型<br> (1) 注意力使用MLB计算（其实啥注意力都差不多)<br> (2) 注意力集成机制：前几层question和video算出注意力权重后，各个问题生成的权重(16,1,1)取平均<br>&emsp;&emsp;与video(16,36,384)各帧相乘<br> (3) attribute 与 question直接算MLB加权平均<br> (4) encode用conv 1<em>1<br>&emsp;&emsp;用faster rcnn特征劣势是失去了空间局部依赖，又因此间接导致失去了物体级的时间局部依赖<br>&emsp;&emsp;rnn与cnn都不能用，采用self-attention+attention稀疏化解决跨时空依赖的想法<br>&emsp;&emsp;稀疏化方式 relu(w-(max(w)+min(w))/(1+λ)<br>&emsp;&emsp;经过调了半天λ，输出注意力矩阵后发现近乎为对角矩阵，退化为1</em>1conv，故直接用1*1conv<br> (6) video与question压成向量用co-attention(其实都差不多)<br> (7) 多问多答问答框架自带的上下文特征，其他question特征取平均与当前question拼接<br> (8) unit调到最小再调dropout（吐槽一下某人的0.95的dropout)<br> (9) 输入层bn + spatial dropout (bn在前，否则会有些shift)<br>&emsp;&emsp;分类层dropout，计算注意力前加spatial dropout<br> (10) focalloss</p></li><li><p>attention + loss 稀疏化(这部分我没用，分数不稳定，上限高下限低)<br> topk注意力，取权重前k名的取平均<br> loss稀疏化，loss最高的k个answer返回梯度，其他不反回<br> 词表太小，loss稀疏化没效果，语言模型如word2vec这种词表近万或过万的有较大效果<br> 可能物体仍然太少，所以不值得稀疏化</p></li></ol><p>参数也不算特别好，经验上关键的地方动了一下，其他没管</p><p>以前不混天池</p><p>未来几个月可能在这边待一会儿</p><p>希望各位大佬发车的时候带上我</p><p>如上</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于PPT上都是图，直接发打字比较好&lt;/p&gt;
&lt;p&gt;PPT那么多页其实也就几段字，哪有那么多东西讲&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;mp4关键帧I帧抽取，抽取I帧至40帧，不足则在I帧之间补足40帧&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2017vqa冠军的模型 faster 
      
    
    </summary>
    
      <category term="数据竞赛" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="QA" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>2017ACL Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/20/taggingScheme/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/20/taggingScheme/</id>
    <published>2018-11-20T07:38:29.000Z</published>
    <updated>2018-11-20T07:44:45.418Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章内容本身其实蛮少的<br>但是意义还是较为重大的</p><p>整篇文章唯一的创新点大概在序列标注的标记模式<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/example.png" alt="这里写图片描述"><br>(1)-(2)-(3)<br>(1)内容使用BIES模式标记实体Begin, Inside, End,Single<br>(2)内容使用类别标记实体类别<br>(3)标记实体的在一对关系中的起始和终止位置</p><p>就近原则组合(2)相同，且(3)分别为1,2的实体</p><p>下面是模型部分<br>直接看图就好，没什么创新点<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/model.png" alt="这里写图片描述"> </p><p>这是loss<br>就是负类正类权重不一样，也没什么好讲的<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/loss.png" alt="这里写图片描述"> </p><p>这种方式效果也较为不错<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/20/taggingScheme/result.png" alt="这里写图片描述"><br>但是弊端也较为明显，对关系重叠问题解决不太好<br>解决方式也很简单，多分类改成多标签分类就好了<br>还有一个貌似对句子拆分要求较高。</p><p>讲讲我做序列标注任务的几点感受<br>对padding十分敏感<br>sota说是crf+lstm<br>其实应该是双层lstm+crf<br>后面太慢了不过可以用cudnn加速版配自己写的mask zero</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章内容本身其实蛮少的&lt;br&gt;但是意义还是较为重大的&lt;/p&gt;
&lt;p&gt;整篇文章唯一的创新点大概在序列标注的标记模式&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/m
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="序列标注" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>同类对比CRF</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/13/%E5%90%8C%E7%B1%BB%E5%AF%B9%E6%AF%94CRF/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/13/同类对比CRF/</id>
    <published>2018-11-13T13:02:41.000Z</published>
    <updated>2018-11-13T13:11:47.150Z</updated>
    
    <content type="html"><![CDATA[<p>很早之前看CRF一直都是一堆概率图<br>一堆公式，看的晕乎乎，最后也不知道怎么算如何用<br>后来几个月前看到苏神的博客才醍醐灌醒<br>然后这几天也一直在研究crf的源码<br>其实这个东西哪里有那么复杂</p><p>同类对比是个很好的理解方式<br>以序列标注为例子<br>首先我们看看直接以softmax建模<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/13/同类对比CRF/softmax.png" alt="这里写图片描述"><br>对序列的每个输出都进行softmax输出概率P(Y<sub>i</sub>|X)<br>整个序列出现概率为P(Y|X) = ∏<sub>i</sub> P(Y<sub>i</sub>|X)<br>该序列的loss为<br>loss = -Σ<sub>i</sub> log(Y<sub>i</sub>|X)</p><p>而这种建模序列概率的方式没有考虑到序列标签的依赖关系<br>crf是考虑了这种依赖关系而对整个句子的概率进行建模<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/13/同类对比CRF/crf.png" alt="这里写图片描述"><br>P(Y|X) = exp(h(y<sub>1</sub>;x)+g(y<sub>1</sub>,y<sub>2</sub>;x)+…+h(y<sub>n</sub>;x))/Z(x)<br>P(Y|X) = exp(Σ<sub>i</sub> h(y<sub>i</sub>;x) + Σ<sub>i</sub>  g(y<sub>i</sub>,y<sub>i+1</sub>;x))/Z(x)<br>如此给句子打分就考虑到了句子间的近临依赖<br>loss = - log(P(Y|X))<br>其中h是之前rnn/cnn等的输出，而g是状态转移矩阵，可用自定义rnn实现</p><p>但是分子计算是完成了<br>分母的计算呢<br>这里用到dp即可计算得到<br>Z<sub>t+1</sub>(i) = ( Σ<sub>j</sub> Z<sub>t</sub>(j) <em> G(j|i) ) </em> H(i|X)<br>Z<sub>t+1</sub>(i)意思是t+1时刻以i结尾的所有序列得分和<br>G(j|i)为exp(g(y<sub>i</sub>,y<sub>i+1</sub>;x))<br>H(i,X)为exp(h(y<sub>i</sub>;x))</p><p>那么还有一个问题，如何预测？<br>普通的对句子的所有输出进行softmax，直接argmax就好了</p><p>用crf的话就涉及到最长路径<br>这个直接用vibiter算法即可获得</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;很早之前看CRF一直都是一堆概率图&lt;br&gt;一堆公式，看的晕乎乎，最后也不知道怎么算如何用&lt;br&gt;后来几个月前看到苏神的博客才醍醐灌醒&lt;br&gt;然后这几天也一直在研究crf的源码&lt;br&gt;其实这个东西哪里有那么复杂&lt;/p&gt;
&lt;p&gt;同类对比是个很好的理解方式&lt;br&gt;以序列标注为例子
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="序列标注" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/"/>
    
  </entry>
  
  <entry>
    <title>IoU-Net 阅读笔记</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/10/iounet/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/10/iounet/</id>
    <published>2018-11-10T05:14:37.000Z</published>
    <updated>2018-11-10T05:41:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>好久没碰检测了<br>前段时间因为一些事情又focus了几天<br>今天做个IOU-Net的简短笔记</p><p>简单来说这篇文章主要是针对这样一个问题<br>检测的目的很明确，为了获得高质量的bounding box<br>然而在以前的做法当中是先通过一系列bounding box然后再经过筛选获得<br>筛选是以分类置信度为第一优先级<br>但是，分类置信度和bbox proposal质量存在相关性但非同步增减<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/iouvsloc.png" alt="这里写图片描述"> </p><p>基于这些问题，这篇文章提出了以下策略</p><h3 id="IoU-predictor"><a href="#IoU-predictor" class="headerlink" title="IoU predictor"></a>IoU predictor</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/ioupredict.png" alt="这里写图片描述"><br>加入IoU预测分支，loss使用smooth L1<br>使用非RPN提供的proposals，而是ground truth加随机绕动得到Jittered RoIs</p><h3 id="IoU-guided-NMS"><a href="#IoU-guided-NMS" class="headerlink" title="IoU-guided NMS"></a>IoU-guided NMS</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/nms.png" alt="这里写图片描述"><br>首先选出IoU最大的bbox<br>获得与该bbox重叠率大于某阈值的bbox集合<br>将该bbox的分类置信度更新为该集合内分类置信度的最大值<br>滤掉该集合内的bbox</p><p>如此循环…</p><h3 id="Bounding-box-refinement-as-an-optimization-procedure"><a href="#Bounding-box-refinement-as-an-optimization-procedure" class="headerlink" title="Bounding box refinement as an optimization procedure"></a>Bounding box refinement as an optimization procedure</h3><p>这部分也就长话短说<br>就是我们现在有一个IoU预测器，以及检测获得的bbox<br>将这些bbox输入，获得其IoU预测值，再通过梯度反向微调bbox</p><h3 id="PrROI直接看公式吧"><a href="#PrROI直接看公式吧" class="headerlink" title="PrROI直接看公式吧"></a>PrROI直接看公式吧</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/prroi1.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/priou2.png" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/priou3.png" alt="这里写图片描述"><br>ROIPool -&gt; ROIAlign 避免量化<br>ROIAlign -&gt; PrROI 修补了ROIAlign不能由bin大小调整的缺陷</p><h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/10/iounet/result.png" alt="这里写图片描述"><br>高IoU阈值时有较显著的提升</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;好久没碰检测了&lt;br&gt;前段时间因为一些事情又focus了几天&lt;br&gt;今天做个IOU-Net的简短笔记&lt;/p&gt;
&lt;p&gt;简单来说这篇文章主要是针对这样一个问题&lt;br&gt;检测的目的很明确，为了获得高质量的bounding box&lt;br&gt;然而在以前的做法当中是先通过一系列boundi
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="目标检测" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>s2s learning as beam-search optimization</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/11/03/s2s/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/11/03/s2s/</id>
    <published>2018-11-03T13:00:51.000Z</published>
    <updated>2018-11-03T13:03:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>这周自己做的水报<br>16年的文章,感觉好像应用的并不多<br>然后文章里的loss缺陷也很明显..约束不足,感觉只适合微调<br>最近真的好忙啊…<br>争取下几周能搞点质量高的<br>反正先发上来</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_2.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_3.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_4.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_5.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_6.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/11/03/s2s/0_7.jpg" alt="这里写图片描述"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这周自己做的水报&lt;br&gt;16年的文章,感觉好像应用的并不多&lt;br&gt;然后文章里的loss缺陷也很明显..约束不足,感觉只适合微调&lt;br&gt;最近真的好忙啊…&lt;br&gt;争取下几周能搞点质量高的&lt;br&gt;反正先发上来&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="seq2seq" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>Memory network</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/21/memorynet/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/21/memorynet/</id>
    <published>2018-10-21T08:23:05.000Z</published>
    <updated>2018-10-21T08:29:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>nlp周报<br>这次给讨论班简述一下memory net的框架<br>ppt上传在这<br>主要口述,字有些少<br>时间限制,篇幅较短</p><p>下次超多版本的attention得好好讲,恩…</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_1.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_2.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_3.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_4.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_5.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_6.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/21/memorynet/0_7.jpg" alt="这里写图片描述"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;nlp周报&lt;br&gt;这次给讨论班简述一下memory net的框架&lt;br&gt;ppt上传在这&lt;br&gt;主要口述,字有些少&lt;br&gt;时间限制,篇幅较短&lt;/p&gt;
&lt;p&gt;下次超多版本的attention得好好讲,恩…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https:
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="QA" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>2018ECCV DaSiamRPN阅读笔记</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/20/dasimarpn/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/20/dasimarpn/</id>
    <published>2018-10-20T06:22:39.000Z</published>
    <updated>2018-10-20T06:29:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文链接<br><a href="https://arxiv.org/pdf/1808.06048.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.06048.pdf</a><br>这个模型是2018VOT实时比赛的冠军,VOT2018长时比赛的亚军<br>DaSiamRPN在普通跟踪的Accuracy指标和长时跟踪的Recall指标中均排名第一</p><p>这个模型是基于他们今年之前的一个模型Siamese RPN改进得到的模型</p><h3 id="处理样本不均衡策略"><a href="#处理样本不均衡策略" class="headerlink" title="处理样本不均衡策略"></a>处理样本不均衡策略</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/problem.jpg" alt="这里写图片描述"><br>作者发现在跟踪过程中跟踪器对实例分类困难<br>而对前背景分类能力较强<br>而造成这个问题的原因他归因为跟踪过程中样本不均衡<br>正样本实例种类不够多模型泛化能力差</p><p>作者在训练过程中加入了如下所示的样本对进行离线训练<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/aug.jpg" alt="这里写图片描述"> </p><h3 id="增量学习方式"><a href="#增量学习方式" class="headerlink" title="增量学习方式"></a>增量学习方式</h3><p>公式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/update.jpg" alt="这里写图片描述"><br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/update2.jpg" alt="这里写图片描述"><br>与模板帧的匹配分数 - 与干扰物的匹配分数作为最终分数<br>这些抗干扰物选择方式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/distractor.jpg" alt="这里写图片描述"><br>选择与模板帧相似度大于某个阈值的错误实例</p><p>再进行一般化<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/update3.jpg" alt="这里写图片描述"> </p><p>参数设置细节见论文</p><h3 id="各组件收益"><a href="#各组件收益" class="headerlink" title="各组件收益"></a>各组件收益</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/20/dasimarpn/gain.jpg" alt="这里写图片描述"> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原文链接&lt;br&gt;&lt;a href=&quot;https://arxiv.org/pdf/1808.06048.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1808.06048.pdf&lt;/a&gt;&lt;br&gt;这个模型是20
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="tracking" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>sparsemax</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/18/sparsemax/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/18/sparsemax/</id>
    <published>2018-10-18T08:30:30.000Z</published>
    <updated>2018-10-18T08:56:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是稀疏性的需求而提出了一个带有稀疏特性的归一化函数sparsemax<br>常见的有 sigmoid/tanh -&gt; hard_sigmoid/hard_tanh<br>sparsemax在二维情况即为hard_sigmoid</p><p>我认为稀疏特性是attention以后十分重要的发展方向<br>目前大部分的soft attention都是基于softmax<br>这就带来了一个缺点,每个元素都会对结果产生影响<br>而hard attention又带了难以优化的问题<br>自然而然sparse attention是一个很好的发展方向<br>虽然本篇文章我应用效果并不怎样…<br>但是仍然感觉很有启发意义</p><p>本篇文章稀疏化的方式是通过一个阈值<br>再利用max函数屏蔽掉一部分权重</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/18/sparsemax/sparsemax.jpg" alt="这里写图片描述"><br>其中[x]<sub>+</sub>指代max(x,0)<br>该阈值的好处</p><ol><li>介于min(z)与max(z)之间,保证能屏蔽一部分与留下一部分</li><li>k的选择保证了主要成分的保留</li></ol></blockquote><p>此函数需自定义梯度回传公式<br>梯度回传公式如下</p><blockquote><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/18/sparsemax/thres.jpg" alt="这里写图片描述"><br>其中S(z)为[z-r(z)]<sub>+</sub>中非0的部分<br>这个梯度公式意思很简单<br>即用到的部分回传梯度,没用到的部分梯度为0</p></blockquote><p>sparsemax应用场景</p><ol><li>attention归一化权重</li><li>多分类</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章主要是稀疏性的需求而提出了一个带有稀疏特性的归一化函数sparsemax&lt;br&gt;常见的有 sigmoid/tanh -&amp;gt; hard_sigmoid/hard_tanh&lt;br&gt;sparsemax在二维情况即为hard_sigmoid&lt;/p&gt;
&lt;p&gt;我认为稀疏特性
      
    
    </summary>
    
      <category term="ML&amp;DL" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/ML-DL/"/>
    
    
      <category term="attention" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>本年爆款... Bidirectional Encoder Representations from Transformers</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/14/bert/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/14/bert/</id>
    <published>2018-10-14T08:11:28.000Z</published>
    <updated>2018-10-14T08:19:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>这几天被BERT刷屏了<br>结果是真的好看,刷新了十一项记录,每项都有巨大改进…<br>下面分析一下这篇文章的工作</p><h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/14/bert/input.jpg" alt="这里写图片描述"><br>一个句子对,即两个句子</p><ol><li>WordPiece Embedding<br>&emsp;这个东西是用来解决oov的word的,将部分单词拆成两个字词,如fued拆成fu,ed .<br>&emsp;具体怎么拆,拆哪些,用贪心算法搜索尽可能少的token去覆盖所有单词</li><li>Segment Embedding<br>&emsp;区分是句子一还是句子二</li><li>Position Embedding<br>&emsp;融入位置信息,学习得到</li><li>cls是句子的类别信息,用于task2和其他任务(非分类任务可无视)</li><li>[SEP]是句子的分隔符<br><br></li></ol><p>下面是该论文的自监督任务的两个创新点</p><h3 id="TASK1-MLM-masked-language-model"><a href="#TASK1-MLM-masked-language-model" class="headerlink" title="TASK1 #: MLM(masked language model)"></a>TASK1 #: MLM(masked language model)</h3><p>随机屏蔽batchsize samples中一定量的单词,并去预测他 (完形填空)</p><ol><li>使用bidirectional self-attention<br>&emsp;不使用rnn可能是因为网络很深不好训练.<br>&emsp;作者认为用单项模型然后两向使用再拼接不如直接双向来的自然,能更好的捕捉上下文信息</li><li>每次屏蔽每个句子中15%的单词<br>&emsp;(1)80%的概率将单词换为[mask]标记 , my dog is hairy → my dog is [MASK]<br>&emsp;(2)10%的概率将单词换为字表中其他单词 , my dog is hairy → my dog is apple<br>&emsp;(3)10%的概率不替换 , my dog is hairy → my dog is hairy<br>具体为什么不是很清楚..   (2)可能是加噪缓解过拟合,(3)不知道了…<br><br></li></ol><h3 id="TASK2-NSP-Next-Sentence-Prediction"><a href="#TASK2-NSP-Next-Sentence-Prediction" class="headerlink" title="TASK2 #: NSP(Next Sentence Prediction)"></a>TASK2 #: NSP(Next Sentence Prediction)</h3><p>由于输入的是一个句子对,所以这个任务是去判断句子二是否可作为句子一的下一句<br>这个任务的目的应该是学习句子之间的逻辑关系</p><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p>下图是在其他任务的微调方式<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/14/bert/fune.jpg" alt="这里写图片描述"> </p><p>由于每次只屏蔽一部分<br>这比left2right收敛慢很多<br>一般来讲自监督的模型一般有比较好的泛化效果<br>然后结果也很惊人…</p><p>最后就是模型好大…用不起用不起…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这几天被BERT刷屏了&lt;br&gt;结果是真的好看,刷新了十一项记录,每项都有巨大改进…&lt;br&gt;下面分析一下这篇文章的工作&lt;/p&gt;
&lt;h3 id=&quot;Input&quot;&gt;&lt;a href=&quot;#Input&quot; class=&quot;headerlink&quot; title=&quot;Input&quot;&gt;&lt;/a&gt;Input
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="representation learning" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/representation-learning/"/>
    
  </entry>
  
  <entry>
    <title>NAACL2018 best paper ELMo</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/13/elmo/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/13/elmo/</id>
    <published>2018-10-13T07:23:00.000Z</published>
    <updated>2018-10-13T08:00:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>ELMo 是NAACL2018的best paper<br>早就想读了了,攒着一直没读…</p><p>其实nn的文章看图能识个大概了,接着再细读其中细节<br>但这篇文章没图…</p><p>下面上一张自制的</p><h3 id="ELMo结构与使用方式"><a href="#ELMo结构与使用方式" class="headerlink" title="ELMo结构与使用方式"></a>ELMo结构与使用方式</h3><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/elmo.jpg" alt="这里写图片描述"> </p><p>如图,上面是ELMo的使用方式<br>将模型中word在LSTM中输出的中间态作为他的embedding</p><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/target.jpg" alt="这里写图片描述"><br>他自己则是一个多层双向语言自监督模型<br>正向预测和逆向预测word作为task,进行训练<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/loss.jpg" alt="这里写图片描述"> </p><p>这种设计个人认为有如下几点好处</p><ol><li>word在不同语境有不同意思,使用LSTM中间状态带入了上下文信息解决了语义歧义的问题</li><li>biLSTM相比Glove,word2vec带入了语序信息</li><li>biLSTM能捕捉一定的语法结构信息</li></ol><h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><p>冷冻biLM模型<br>将他各个中间层的信息加权平均,再和普通的词向量concat<br>公式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/13/elmo/w.jpg" alt="这里写图片描述"> </p><p>s<sub>task</sub>是可训练且经过softmax归一化的权重<br>s<sub>task</sub>用于在不同任务下自适应调整高维还是低维的抽象信息<br>γ是缩放因子,对模型影响较大,可训练</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ELMo 是NAACL2018的best paper&lt;br&gt;早就想读了了,攒着一直没读…&lt;/p&gt;
&lt;p&gt;其实nn的文章看图能识个大概了,接着再细读其中细节&lt;br&gt;但这篇文章没图…&lt;/p&gt;
&lt;p&gt;下面上一张自制的&lt;/p&gt;
&lt;h3 id=&quot;ELMo结构与使用方式&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/NLP/"/>
    
    
      <category term="representation learning" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/representation-learning/"/>
    
  </entry>
  
  <entry>
    <title>Object Context Network for Scene Parsing</title>
    <link href="https://github.com/qrfaction/qrfaction.github.io/2018/10/02/OCnet/"/>
    <id>https://github.com/qrfaction/qrfaction.github.io/2018/10/02/OCnet/</id>
    <published>2018-10-02T14:34:20.000Z</published>
    <updated>2018-10-05T07:22:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇应用了attention在图像分割的文章<br>文章本身很简单,感觉工作不多<br>最近在vqa工作中对attention体会很深,也创新了不少东西<br>在结束后再写篇博文吧</p><p>回到正文</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/02/OCnet/total.jpg" alt="这里写图片描述"> </p><p>恩 … 整体框架还是无特别大的创新</p><h2 id="Object-Context"><a href="#Object-Context" class="headerlink" title="Object Context"></a>Object Context</h2><p><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/02/OCnet/attention.jpg" alt="这里写图片描述"> </p><p>输出的C指context<br>P指Position embedding<br>X+P将位置信息融入feature map<br>再做个很普通的attention<br>就是以像素为单位,以余弦相关性作为相似度度量.<br>获得了context他的使用方式如下<br><img src="https://raw.githubusercontent.com/qrfaction/qrfaction.github.io/master/2018/10/02/OCnet/oc.jpg" alt="这里写图片描述"><br>配合hypercolumn或ASP</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>其实感觉Context插入的很强行<br>self-attention其实有跨大距离依赖效果才比conv强的<br>相比于self-attention<br>分割这种感受野任务,给channel加attention(不同感受野feature map拼接后权重不同)效果更佳<br>效果也只有几个千分点的提升,表示质疑<br>朋友测试下效果也不是很好…</p><p>而且他的注意力是直接用softmax的<br>对于这么多像素做softmax  有效信息被无效信息覆盖的情况一般都很严重…<br>我严重怀疑这个和average效果差不多<br>他可以试试半hard半soft的attention</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是一篇应用了attention在图像分割的文章&lt;br&gt;文章本身很简单,感觉工作不多&lt;br&gt;最近在vqa工作中对attention体会很深,也创新了不少东西&lt;br&gt;在结束后再写篇博文吧&lt;/p&gt;
&lt;p&gt;回到正文&lt;/p&gt;
&lt;h2 id=&quot;整体架构&quot;&gt;&lt;a href=&quot;#整体架构
      
    
    </summary>
    
      <category term="CV" scheme="https://github.com/qrfaction/qrfaction.github.io/categories/CV/"/>
    
    
      <category term="图像分割" scheme="https://github.com/qrfaction/qrfaction.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
</feed>
